{
  "results": {
    "bbh_flan_cot_zeroshot": {
      "exact_match,get-answer": 0.12885885424665947,
      "exact_match_stderr,get-answer": 0.14326273742629245
    },
    " - bbh_flan_cot_zeroshot_boolean_expressions": {
      "exact_match,get-answer": 0.536,
      "exact_match_stderr,get-answer": 0.031603975145223735
    },
    " - bbh_flan_cot_zeroshot_causal_judgement": {
      "exact_match,get-answer": 0.1443850267379679,
      "exact_match_stderr,get-answer": 0.025771743173342163
    },
    " - bbh_flan_cot_zeroshot_date_understanding": {
      "exact_match,get-answer": 0.16,
      "exact_match_stderr,get-answer": 0.023232714782060668
    },
    " - bbh_flan_cot_zeroshot_disambiguation_qa": {
      "exact_match,get-answer": 0.296,
      "exact_match_stderr,get-answer": 0.02892893938837962
    },
    " - bbh_flan_cot_zeroshot_dyck_languages": {
      "exact_match,get-answer": 0.0,
      "exact_match_stderr,get-answer": 0.0
    },
    " - bbh_flan_cot_zeroshot_formal_fallacies": {
      "exact_match,get-answer": 0.064,
      "exact_match_stderr,get-answer": 0.015510587134374148
    },
    " - bbh_flan_cot_zeroshot_geometric_shapes": {
      "exact_match,get-answer": 0.0,
      "exact_match_stderr,get-answer": 0.0
    },
    " - bbh_flan_cot_zeroshot_hyperbaton": {
      "exact_match,get-answer": 0.0,
      "exact_match_stderr,get-answer": 0.0
    },
    " - bbh_flan_cot_zeroshot_logical_deduction_five_objects": {
      "exact_match,get-answer": 0.088,
      "exact_match_stderr,get-answer": 0.017953084777052864
    },
    " - bbh_flan_cot_zeroshot_logical_deduction_seven_objects": {
      "exact_match,get-answer": 0.064,
      "exact_match_stderr,get-answer": 0.015510587134374143
    },
    " - bbh_flan_cot_zeroshot_logical_deduction_three_objects": {
      "exact_match,get-answer": 0.324,
      "exact_match_stderr,get-answer": 0.02965829492454557
    },
    " - bbh_flan_cot_zeroshot_movie_recommendation": {
      "exact_match,get-answer": 0.188,
      "exact_match_stderr,get-answer": 0.02476037772775051
    },
    " - bbh_flan_cot_zeroshot_multistep_arithmetic_two": {
      "exact_match,get-answer": 0.0,
      "exact_match_stderr,get-answer": 0.0
    },
    " - bbh_flan_cot_zeroshot_navigate": {
      "exact_match,get-answer": 0.54,
      "exact_match_stderr,get-answer": 0.03158465389149899
    },
    " - bbh_flan_cot_zeroshot_object_counting": {
      "exact_match,get-answer": 0.124,
      "exact_match_stderr,get-answer": 0.02088638225867326
    },
    " - bbh_flan_cot_zeroshot_penguins_in_a_table": {
      "exact_match,get-answer": 0.1917808219178082,
      "exact_match_stderr,get-answer": 0.03269513706984762
    },
    " - bbh_flan_cot_zeroshot_reasoning_about_colored_objects": {
      "exact_match,get-answer": 0.092,
      "exact_match_stderr,get-answer": 0.018316275379429644
    },
    " - bbh_flan_cot_zeroshot_ruin_names": {
      "exact_match,get-answer": 0.076,
      "exact_match_stderr,get-answer": 0.016793573067859627
    },
    " - bbh_flan_cot_zeroshot_salient_translation_error_detection": {
      "exact_match,get-answer": 0.0,
      "exact_match_stderr,get-answer": 0.0
    },
    " - bbh_flan_cot_zeroshot_snarks": {
      "exact_match,get-answer": 0.3595505617977528,
      "exact_match_stderr,get-answer": 0.036069139140740274
    },
    " - bbh_flan_cot_zeroshot_sports_understanding": {
      "exact_match,get-answer": 0.104,
      "exact_match_stderr,get-answer": 0.019345100974843883
    },
    " - bbh_flan_cot_zeroshot_temporal_sequences": {
      "exact_match,get-answer": 0.156,
      "exact_match_stderr,get-answer": 0.022995023034068758
    },
    " - bbh_flan_cot_zeroshot_tracking_shuffled_objects_five_objects": {
      "exact_match,get-answer": 0.04,
      "exact_match_stderr,get-answer": 0.012418408411301306
    },
    " - bbh_flan_cot_zeroshot_tracking_shuffled_objects_seven_objects": {
      "exact_match,get-answer": 0.028,
      "exact_match_stderr,get-answer": 0.010454721651927292
    },
    " - bbh_flan_cot_zeroshot_tracking_shuffled_objects_three_objects": {
      "exact_match,get-answer": 0.0,
      "exact_match_stderr,get-answer": 0.0
    },
    " - bbh_flan_cot_zeroshot_web_of_lies": {
      "exact_match,get-answer": 0.0,
      "exact_match_stderr,get-answer": 0.0
    },
    " - bbh_flan_cot_zeroshot_word_sorting": {
      "exact_match,get-answer": 0.0,
      "exact_match_stderr,get-answer": 0.0
    }
  },
  "groups": {
    "bbh_flan_cot_zeroshot": {
      "exact_match,get-answer": 0.12885885424665947,
      "exact_match_stderr,get-answer": 0.14326273742629245
    }
  },
  "configs": {
    "bbh_flan_cot_zeroshot_boolean_expressions": {
      "task": "bbh_flan_cot_zeroshot_boolean_expressions",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "boolean_expressions",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Evaluate the result of a random Boolean expression.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_causal_judgement": {
      "task": "bbh_flan_cot_zeroshot_causal_judgement",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "causal_judgement",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Answer questions about causal attribution.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_date_understanding": {
      "task": "bbh_flan_cot_zeroshot_date_understanding",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "date_understanding",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Infer the date from context.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_disambiguation_qa": {
      "task": "bbh_flan_cot_zeroshot_disambiguation_qa",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "disambiguation_qa",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Clarify the meaning of sentences with ambiguous pronouns.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_dyck_languages": {
      "task": "bbh_flan_cot_zeroshot_dyck_languages",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "dyck_languages",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Correctly close a Dyck-n word.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_formal_fallacies": {
      "task": "bbh_flan_cot_zeroshot_formal_fallacies",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "formal_fallacies",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Distinguish deductively valid arguments from formal fallacies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_geometric_shapes": {
      "task": "bbh_flan_cot_zeroshot_geometric_shapes",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "geometric_shapes",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Name geometric shapes from their SVG paths.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_hyperbaton": {
      "task": "bbh_flan_cot_zeroshot_hyperbaton",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "hyperbaton",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Order adjectives correctly in English sentences.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_logical_deduction_five_objects": {
      "task": "bbh_flan_cot_zeroshot_logical_deduction_five_objects",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "logical_deduction_five_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "A logical deduction task which requires deducing the order of a sequence of objects.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_logical_deduction_seven_objects": {
      "task": "bbh_flan_cot_zeroshot_logical_deduction_seven_objects",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "logical_deduction_seven_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "A logical deduction task which requires deducing the order of a sequence of objects.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_logical_deduction_three_objects": {
      "task": "bbh_flan_cot_zeroshot_logical_deduction_three_objects",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "logical_deduction_three_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "A logical deduction task which requires deducing the order of a sequence of objects.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_movie_recommendation": {
      "task": "bbh_flan_cot_zeroshot_movie_recommendation",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "movie_recommendation",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Recommend movies similar to the given list of movies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_multistep_arithmetic_two": {
      "task": "bbh_flan_cot_zeroshot_multistep_arithmetic_two",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "multistep_arithmetic_two",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Solve multi-step arithmetic problems.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_navigate": {
      "task": "bbh_flan_cot_zeroshot_navigate",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "navigate",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Given a series of navigation instructions, determine whether one would end up back at the starting point.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_object_counting": {
      "task": "bbh_flan_cot_zeroshot_object_counting",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "object_counting",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Questions that involve enumerating objects and asking the model to count them.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_penguins_in_a_table": {
      "task": "bbh_flan_cot_zeroshot_penguins_in_a_table",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "penguins_in_a_table",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Answer questions about a table of penguins and their attributes.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_reasoning_about_colored_objects": {
      "task": "bbh_flan_cot_zeroshot_reasoning_about_colored_objects",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "reasoning_about_colored_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Answer extremely simple questions about the colors of objects on a surface.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_ruin_names": {
      "task": "bbh_flan_cot_zeroshot_ruin_names",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "ruin_names",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Select the humorous edit that 'ruins' the input movie or musical artist name.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_salient_translation_error_detection": {
      "task": "bbh_flan_cot_zeroshot_salient_translation_error_detection",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "salient_translation_error_detection",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Detect the type of error in an English translation of a German source sentence.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_snarks": {
      "task": "bbh_flan_cot_zeroshot_snarks",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "snarks",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Determine which of two sentences is sarcastic.\n\nAccording to Cambridge University Dictionary, sarcasm is \"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way.\" Sarcastic sentences often contain satirical or ironic utterances, hyperboles, ambivalent or witty remarks.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_sports_understanding": {
      "task": "bbh_flan_cot_zeroshot_sports_understanding",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "sports_understanding",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Determine whether an artificially constructed sentence relating to sports is plausible or not.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_temporal_sequences": {
      "task": "bbh_flan_cot_zeroshot_temporal_sequences",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "temporal_sequences",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Task description: Answer questions about which times certain events could have occurred.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_tracking_shuffled_objects_five_objects": {
      "task": "bbh_flan_cot_zeroshot_tracking_shuffled_objects_five_objects",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "tracking_shuffled_objects_five_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_tracking_shuffled_objects_seven_objects": {
      "task": "bbh_flan_cot_zeroshot_tracking_shuffled_objects_seven_objects",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "tracking_shuffled_objects_seven_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_tracking_shuffled_objects_three_objects": {
      "task": "bbh_flan_cot_zeroshot_tracking_shuffled_objects_three_objects",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "tracking_shuffled_objects_three_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_web_of_lies": {
      "task": "bbh_flan_cot_zeroshot_web_of_lies",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "web_of_lies",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Evaluate a random boolean function expressed as a word problem.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    },
    "bbh_flan_cot_zeroshot_word_sorting": {
      "task": "bbh_flan_cot_zeroshot_word_sorting",
      "group": "bbh_flan_cot_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "word_sorting",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA: Let's think step by step.\n",
      "doc_to_target": "{{target}}",
      "description": "Sort a list of words.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "filter_list": [
        {
          "name": "get-answer",
          "filter": [
            {
              "function": "regex",
              "regex_pattern": "((?<=The answer is )(.*)(?=.)|(?<=the answer is )(.*)(?=.)|(?<=The answer: )(.*)(?=.)|(?<=The final answer: )(.*)(?=.))"
            },
            {
              "function": "take_first"
            }
          ]
        }
      ],
      "should_decontaminate": false
    }
  },
  "versions": {
    " - bbh_flan_cot_zeroshot_boolean_expressions": "Yaml",
    " - bbh_flan_cot_zeroshot_causal_judgement": "Yaml",
    " - bbh_flan_cot_zeroshot_date_understanding": "Yaml",
    " - bbh_flan_cot_zeroshot_disambiguation_qa": "Yaml",
    " - bbh_flan_cot_zeroshot_dyck_languages": "Yaml",
    " - bbh_flan_cot_zeroshot_formal_fallacies": "Yaml",
    " - bbh_flan_cot_zeroshot_geometric_shapes": "Yaml",
    " - bbh_flan_cot_zeroshot_hyperbaton": "Yaml",
    " - bbh_flan_cot_zeroshot_logical_deduction_five_objects": "Yaml",
    " - bbh_flan_cot_zeroshot_logical_deduction_seven_objects": "Yaml",
    " - bbh_flan_cot_zeroshot_logical_deduction_three_objects": "Yaml",
    " - bbh_flan_cot_zeroshot_movie_recommendation": "Yaml",
    " - bbh_flan_cot_zeroshot_multistep_arithmetic_two": "Yaml",
    " - bbh_flan_cot_zeroshot_navigate": "Yaml",
    " - bbh_flan_cot_zeroshot_object_counting": "Yaml",
    " - bbh_flan_cot_zeroshot_penguins_in_a_table": "Yaml",
    " - bbh_flan_cot_zeroshot_reasoning_about_colored_objects": "Yaml",
    " - bbh_flan_cot_zeroshot_ruin_names": "Yaml",
    " - bbh_flan_cot_zeroshot_salient_translation_error_detection": "Yaml",
    " - bbh_flan_cot_zeroshot_snarks": "Yaml",
    " - bbh_flan_cot_zeroshot_sports_understanding": "Yaml",
    " - bbh_flan_cot_zeroshot_temporal_sequences": "Yaml",
    " - bbh_flan_cot_zeroshot_tracking_shuffled_objects_five_objects": "Yaml",
    " - bbh_flan_cot_zeroshot_tracking_shuffled_objects_seven_objects": "Yaml",
    " - bbh_flan_cot_zeroshot_tracking_shuffled_objects_three_objects": "Yaml",
    " - bbh_flan_cot_zeroshot_web_of_lies": "Yaml",
    " - bbh_flan_cot_zeroshot_word_sorting": "Yaml",
    "bbh_flan_cot_zeroshot": "N/A"
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=/fsx/lintangsutawika/00-improved-t5/checkpoints/hf/t5-v2-base-2M-flan",
    "batch_size": "4",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000
  },
  "git_hash": "b439c98"
}