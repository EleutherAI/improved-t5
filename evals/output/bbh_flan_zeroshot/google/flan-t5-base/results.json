{
  "results": {
    "bbh_flan_zeroshot": {
      "exact_match,none": 0.15542927353709107,
      "exact_match_stderr,none": 0.12645623490471394
    },
    " - bbh_flan_zeroshot_boolean_expressions": {
      "exact_match,none": 0.336,
      "exact_match_stderr,none": 0.02993325909419152
    },
    " - bbh_flan_zeroshot_causal_judgement": {
      "exact_match,none": 0.5240641711229946,
      "exact_match_stderr,none": 0.03661929361528702
    },
    " - bbh_flan_zeroshot_date_understanding": {
      "exact_match,none": 0.024,
      "exact_match_stderr,none": 0.009699087026964266
    },
    " - bbh_flan_zeroshot_disambiguation_qa": {
      "exact_match,none": 0.012,
      "exact_match_stderr,none": 0.006900323023694279
    },
    " - bbh_flan_zeroshot_dyck_languages": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_formal_fallacies": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_geometric_shapes": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_hyperbaton": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_logical_deduction_five_objects": {
      "exact_match,none": 0.296,
      "exact_match_stderr,none": 0.028928939388379617
    },
    " - bbh_flan_zeroshot_logical_deduction_seven_objects": {
      "exact_match,none": 0.22,
      "exact_match_stderr,none": 0.026251792824605845
    },
    " - bbh_flan_zeroshot_logical_deduction_three_objects": {
      "exact_match,none": 0.372,
      "exact_match_stderr,none": 0.030630325944558313
    },
    " - bbh_flan_zeroshot_movie_recommendation": {
      "exact_match,none": 0.264,
      "exact_match_stderr,none": 0.027934518957690908
    },
    " - bbh_flan_zeroshot_multistep_arithmetic_two": {
      "exact_match,none": 0.004,
      "exact_match_stderr,none": 0.004000000000000008
    },
    " - bbh_flan_zeroshot_navigate": {
      "exact_match,none": 0.42,
      "exact_match_stderr,none": 0.03127799950463661
    },
    " - bbh_flan_zeroshot_object_counting": {
      "exact_match,none": 0.24,
      "exact_match_stderr,none": 0.027065293652239003
    },
    " - bbh_flan_zeroshot_penguins_in_a_table": {
      "exact_match,none": 0.0547945205479452,
      "exact_match_stderr,none": 0.018899385333166003
    },
    " - bbh_flan_zeroshot_reasoning_about_colored_objects": {
      "exact_match,none": 0.012,
      "exact_match_stderr,none": 0.006900323023694285
    },
    " - bbh_flan_zeroshot_ruin_names": {
      "exact_match,none": 0.216,
      "exact_match_stderr,none": 0.02607865766373273
    },
    " - bbh_flan_zeroshot_salient_translation_error_detection": {
      "exact_match,none": 0.072,
      "exact_match_stderr,none": 0.016381005750490105
    },
    " - bbh_flan_zeroshot_snarks": {
      "exact_match,none": 0.19101123595505617,
      "exact_match_stderr,none": 0.029547046053276616
    },
    " - bbh_flan_zeroshot_sports_understanding": {
      "exact_match,none": 0.192,
      "exact_match_stderr,none": 0.024960691989172012
    },
    " - bbh_flan_zeroshot_temporal_sequences": {
      "exact_match,none": 0.136,
      "exact_match_stderr,none": 0.021723342617052062
    },
    " - bbh_flan_zeroshot_tracking_shuffled_objects_five_objects": {
      "exact_match,none": 0.144,
      "exact_match_stderr,none": 0.022249407735450207
    },
    " - bbh_flan_zeroshot_tracking_shuffled_objects_seven_objects": {
      "exact_match,none": 0.092,
      "exact_match_stderr,none": 0.018316275379429644
    },
    " - bbh_flan_zeroshot_tracking_shuffled_objects_three_objects": {
      "exact_match,none": 0.24,
      "exact_match_stderr,none": 0.027065293652239003
    },
    " - bbh_flan_zeroshot_web_of_lies": {
      "exact_match,none": 0.16,
      "exact_match_stderr,none": 0.02323271478206066
    },
    " - bbh_flan_zeroshot_word_sorting": {
      "exact_match,none": 0.036,
      "exact_match_stderr,none": 0.011805655169278133
    }
  },
  "groups": {
    "bbh_flan_zeroshot": {
      "exact_match,none": 0.15542927353709107,
      "exact_match_stderr,none": 0.12645623490471394
    }
  },
  "configs": {
    "bbh_flan_zeroshot_boolean_expressions": {
      "task": "bbh_flan_zeroshot_boolean_expressions",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "boolean_expressions",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Evaluate the result of a random Boolean expression.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_causal_judgement": {
      "task": "bbh_flan_zeroshot_causal_judgement",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "causal_judgement",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Answer questions about causal attribution.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_date_understanding": {
      "task": "bbh_flan_zeroshot_date_understanding",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "date_understanding",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Infer the date from context.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_disambiguation_qa": {
      "task": "bbh_flan_zeroshot_disambiguation_qa",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "disambiguation_qa",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Clarify the meaning of sentences with ambiguous pronouns.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_dyck_languages": {
      "task": "bbh_flan_zeroshot_dyck_languages",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "dyck_languages",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Correctly close a Dyck-n word.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_formal_fallacies": {
      "task": "bbh_flan_zeroshot_formal_fallacies",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "formal_fallacies",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Distinguish deductively valid arguments from formal fallacies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_geometric_shapes": {
      "task": "bbh_flan_zeroshot_geometric_shapes",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "geometric_shapes",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Name geometric shapes from their SVG paths.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_hyperbaton": {
      "task": "bbh_flan_zeroshot_hyperbaton",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "hyperbaton",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Order adjectives correctly in English sentences.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_logical_deduction_five_objects": {
      "task": "bbh_flan_zeroshot_logical_deduction_five_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "logical_deduction_five_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A logical deduction task which requires deducing the order of a sequence of objects.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_logical_deduction_seven_objects": {
      "task": "bbh_flan_zeroshot_logical_deduction_seven_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "logical_deduction_seven_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A logical deduction task which requires deducing the order of a sequence of objects.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_logical_deduction_three_objects": {
      "task": "bbh_flan_zeroshot_logical_deduction_three_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "logical_deduction_three_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A logical deduction task which requires deducing the order of a sequence of objects.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_movie_recommendation": {
      "task": "bbh_flan_zeroshot_movie_recommendation",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "movie_recommendation",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Recommend movies similar to the given list of movies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_multistep_arithmetic_two": {
      "task": "bbh_flan_zeroshot_multistep_arithmetic_two",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "multistep_arithmetic_two",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Solve multi-step arithmetic problems.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_navigate": {
      "task": "bbh_flan_zeroshot_navigate",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "navigate",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Given a series of navigation instructions, determine whether one would end up back at the starting point.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_object_counting": {
      "task": "bbh_flan_zeroshot_object_counting",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "object_counting",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Questions that involve enumerating objects and asking the model to count them.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_penguins_in_a_table": {
      "task": "bbh_flan_zeroshot_penguins_in_a_table",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "penguins_in_a_table",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Answer questions about a table of penguins and their attributes.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_reasoning_about_colored_objects": {
      "task": "bbh_flan_zeroshot_reasoning_about_colored_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "reasoning_about_colored_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Answer extremely simple questions about the colors of objects on a surface.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_ruin_names": {
      "task": "bbh_flan_zeroshot_ruin_names",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "ruin_names",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Select the humorous edit that 'ruins' the input movie or musical artist name.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_salient_translation_error_detection": {
      "task": "bbh_flan_zeroshot_salient_translation_error_detection",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "salient_translation_error_detection",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Detect the type of error in an English translation of a German source sentence.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_snarks": {
      "task": "bbh_flan_zeroshot_snarks",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "snarks",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Determine which of two sentences is sarcastic.\n\nAccording to Cambridge University Dictionary, sarcasm is \"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way.\" Sarcastic sentences often contain satirical or ironic utterances, hyperboles, ambivalent or witty remarks.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_sports_understanding": {
      "task": "bbh_flan_zeroshot_sports_understanding",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "sports_understanding",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Determine whether an artificially constructed sentence relating to sports is plausible or not.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_temporal_sequences": {
      "task": "bbh_flan_zeroshot_temporal_sequences",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "temporal_sequences",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Task description: Answer questions about which times certain events could have occurred.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_tracking_shuffled_objects_five_objects": {
      "task": "bbh_flan_zeroshot_tracking_shuffled_objects_five_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "tracking_shuffled_objects_five_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_tracking_shuffled_objects_seven_objects": {
      "task": "bbh_flan_zeroshot_tracking_shuffled_objects_seven_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "tracking_shuffled_objects_seven_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_tracking_shuffled_objects_three_objects": {
      "task": "bbh_flan_zeroshot_tracking_shuffled_objects_three_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "tracking_shuffled_objects_three_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_web_of_lies": {
      "task": "bbh_flan_zeroshot_web_of_lies",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "web_of_lies",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Evaluate a random boolean function expressed as a word problem.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_word_sorting": {
      "task": "bbh_flan_zeroshot_word_sorting",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "word_sorting",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Sort a list of words.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    }
  },
  "versions": {
    " - bbh_flan_zeroshot_boolean_expressions": "Yaml",
    " - bbh_flan_zeroshot_causal_judgement": "Yaml",
    " - bbh_flan_zeroshot_date_understanding": "Yaml",
    " - bbh_flan_zeroshot_disambiguation_qa": "Yaml",
    " - bbh_flan_zeroshot_dyck_languages": "Yaml",
    " - bbh_flan_zeroshot_formal_fallacies": "Yaml",
    " - bbh_flan_zeroshot_geometric_shapes": "Yaml",
    " - bbh_flan_zeroshot_hyperbaton": "Yaml",
    " - bbh_flan_zeroshot_logical_deduction_five_objects": "Yaml",
    " - bbh_flan_zeroshot_logical_deduction_seven_objects": "Yaml",
    " - bbh_flan_zeroshot_logical_deduction_three_objects": "Yaml",
    " - bbh_flan_zeroshot_movie_recommendation": "Yaml",
    " - bbh_flan_zeroshot_multistep_arithmetic_two": "Yaml",
    " - bbh_flan_zeroshot_navigate": "Yaml",
    " - bbh_flan_zeroshot_object_counting": "Yaml",
    " - bbh_flan_zeroshot_penguins_in_a_table": "Yaml",
    " - bbh_flan_zeroshot_reasoning_about_colored_objects": "Yaml",
    " - bbh_flan_zeroshot_ruin_names": "Yaml",
    " - bbh_flan_zeroshot_salient_translation_error_detection": "Yaml",
    " - bbh_flan_zeroshot_snarks": "Yaml",
    " - bbh_flan_zeroshot_sports_understanding": "Yaml",
    " - bbh_flan_zeroshot_temporal_sequences": "Yaml",
    " - bbh_flan_zeroshot_tracking_shuffled_objects_five_objects": "Yaml",
    " - bbh_flan_zeroshot_tracking_shuffled_objects_seven_objects": "Yaml",
    " - bbh_flan_zeroshot_tracking_shuffled_objects_three_objects": "Yaml",
    " - bbh_flan_zeroshot_web_of_lies": "Yaml",
    " - bbh_flan_zeroshot_word_sorting": "Yaml",
    "bbh_flan_zeroshot": "N/A"
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=google/flan-t5-base",
    "batch_size": "4",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000
  },
  "git_hash": "b439c98"
}