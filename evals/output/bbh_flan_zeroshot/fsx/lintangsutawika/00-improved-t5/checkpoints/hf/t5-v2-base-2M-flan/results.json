{
  "results": {
    "bbh_flan_zeroshot": {
      "exact_match,none": 0.10643526340039933,
      "exact_match_stderr,none": 0.1606332409357264
    },
    " - bbh_flan_zeroshot_boolean_expressions": {
      "exact_match,none": 0.484,
      "exact_match_stderr,none": 0.03166998503010742
    },
    " - bbh_flan_zeroshot_causal_judgement": {
      "exact_match,none": 0.5401069518716578,
      "exact_match_stderr,none": 0.03654364252047575
    },
    " - bbh_flan_zeroshot_date_understanding": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_disambiguation_qa": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_dyck_languages": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_formal_fallacies": {
      "exact_match,none": 0.524,
      "exact_match_stderr,none": 0.03164968895968782
    },
    " - bbh_flan_zeroshot_geometric_shapes": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_hyperbaton": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_logical_deduction_five_objects": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_logical_deduction_seven_objects": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_logical_deduction_three_objects": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_movie_recommendation": {
      "exact_match,none": 0.016,
      "exact_match_stderr,none": 0.007951661188874314
    },
    " - bbh_flan_zeroshot_multistep_arithmetic_two": {
      "exact_match,none": 0.02,
      "exact_match_stderr,none": 0.008872139507342681
    },
    " - bbh_flan_zeroshot_navigate": {
      "exact_match,none": 0.4,
      "exact_match_stderr,none": 0.031046021028253244
    },
    " - bbh_flan_zeroshot_object_counting": {
      "exact_match,none": 0.28,
      "exact_match_stderr,none": 0.028454148277832332
    },
    " - bbh_flan_zeroshot_penguins_in_a_table": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_reasoning_about_colored_objects": {
      "exact_match,none": 0.096,
      "exact_match_stderr,none": 0.01866896141947718
    },
    " - bbh_flan_zeroshot_ruin_names": {
      "exact_match,none": 0.008,
      "exact_match_stderr,none": 0.0056454836766901715
    },
    " - bbh_flan_zeroshot_salient_translation_error_detection": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_snarks": {
      "exact_match,none": 0.0056179775280898875,
      "exact_match_stderr,none": 0.0056179775280898875
    },
    " - bbh_flan_zeroshot_sports_understanding": {
      "exact_match,none": 0.512,
      "exact_match_stderr,none": 0.03167708558254708
    },
    " - bbh_flan_zeroshot_temporal_sequences": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_tracking_shuffled_objects_five_objects": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_tracking_shuffled_objects_seven_objects": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_tracking_shuffled_objects_three_objects": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    " - bbh_flan_zeroshot_web_of_lies": {
      "exact_match,none": 0.008,
      "exact_match_stderr,none": 0.005645483676690164
    },
    " - bbh_flan_zeroshot_word_sorting": {
      "exact_match,none": 0.016,
      "exact_match_stderr,none": 0.007951661188874347
    }
  },
  "groups": {
    "bbh_flan_zeroshot": {
      "exact_match,none": 0.10643526340039933,
      "exact_match_stderr,none": 0.1606332409357264
    }
  },
  "configs": {
    "bbh_flan_zeroshot_boolean_expressions": {
      "task": "bbh_flan_zeroshot_boolean_expressions",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "boolean_expressions",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Evaluate the result of a random Boolean expression.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_causal_judgement": {
      "task": "bbh_flan_zeroshot_causal_judgement",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "causal_judgement",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Answer questions about causal attribution.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_date_understanding": {
      "task": "bbh_flan_zeroshot_date_understanding",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "date_understanding",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Infer the date from context.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_disambiguation_qa": {
      "task": "bbh_flan_zeroshot_disambiguation_qa",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "disambiguation_qa",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Clarify the meaning of sentences with ambiguous pronouns.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_dyck_languages": {
      "task": "bbh_flan_zeroshot_dyck_languages",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "dyck_languages",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Correctly close a Dyck-n word.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_formal_fallacies": {
      "task": "bbh_flan_zeroshot_formal_fallacies",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "formal_fallacies",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Distinguish deductively valid arguments from formal fallacies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_geometric_shapes": {
      "task": "bbh_flan_zeroshot_geometric_shapes",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "geometric_shapes",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Name geometric shapes from their SVG paths.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_hyperbaton": {
      "task": "bbh_flan_zeroshot_hyperbaton",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "hyperbaton",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Order adjectives correctly in English sentences.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_logical_deduction_five_objects": {
      "task": "bbh_flan_zeroshot_logical_deduction_five_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "logical_deduction_five_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A logical deduction task which requires deducing the order of a sequence of objects.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_logical_deduction_seven_objects": {
      "task": "bbh_flan_zeroshot_logical_deduction_seven_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "logical_deduction_seven_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A logical deduction task which requires deducing the order of a sequence of objects.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_logical_deduction_three_objects": {
      "task": "bbh_flan_zeroshot_logical_deduction_three_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "logical_deduction_three_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A logical deduction task which requires deducing the order of a sequence of objects.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_movie_recommendation": {
      "task": "bbh_flan_zeroshot_movie_recommendation",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "movie_recommendation",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Recommend movies similar to the given list of movies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_multistep_arithmetic_two": {
      "task": "bbh_flan_zeroshot_multistep_arithmetic_two",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "multistep_arithmetic_two",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Solve multi-step arithmetic problems.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_navigate": {
      "task": "bbh_flan_zeroshot_navigate",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "navigate",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Given a series of navigation instructions, determine whether one would end up back at the starting point.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_object_counting": {
      "task": "bbh_flan_zeroshot_object_counting",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "object_counting",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Questions that involve enumerating objects and asking the model to count them.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_penguins_in_a_table": {
      "task": "bbh_flan_zeroshot_penguins_in_a_table",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "penguins_in_a_table",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Answer questions about a table of penguins and their attributes.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_reasoning_about_colored_objects": {
      "task": "bbh_flan_zeroshot_reasoning_about_colored_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "reasoning_about_colored_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Answer extremely simple questions about the colors of objects on a surface.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_ruin_names": {
      "task": "bbh_flan_zeroshot_ruin_names",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "ruin_names",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Select the humorous edit that 'ruins' the input movie or musical artist name.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_salient_translation_error_detection": {
      "task": "bbh_flan_zeroshot_salient_translation_error_detection",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "salient_translation_error_detection",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Detect the type of error in an English translation of a German source sentence.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_snarks": {
      "task": "bbh_flan_zeroshot_snarks",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "snarks",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Determine which of two sentences is sarcastic.\n\nAccording to Cambridge University Dictionary, sarcasm is \"the use of remarks that clearly mean the opposite of what they say, made in order to hurt someone's feelings or to criticize something in a humorous way.\" Sarcastic sentences often contain satirical or ironic utterances, hyperboles, ambivalent or witty remarks.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_sports_understanding": {
      "task": "bbh_flan_zeroshot_sports_understanding",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "sports_understanding",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Determine whether an artificially constructed sentence relating to sports is plausible or not.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_temporal_sequences": {
      "task": "bbh_flan_zeroshot_temporal_sequences",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "temporal_sequences",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Task description: Answer questions about which times certain events could have occurred.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_tracking_shuffled_objects_five_objects": {
      "task": "bbh_flan_zeroshot_tracking_shuffled_objects_five_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "tracking_shuffled_objects_five_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_tracking_shuffled_objects_seven_objects": {
      "task": "bbh_flan_zeroshot_tracking_shuffled_objects_seven_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "tracking_shuffled_objects_seven_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_tracking_shuffled_objects_three_objects": {
      "task": "bbh_flan_zeroshot_tracking_shuffled_objects_three_objects",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "tracking_shuffled_objects_three_objects",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "A task requiring determining the final positions of a set of objects given their initial positions and a description of a sequence of swaps.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_web_of_lies": {
      "task": "bbh_flan_zeroshot_web_of_lies",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "web_of_lies",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Evaluate a random boolean function expressed as a word problem.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "bbh_flan_zeroshot_word_sorting": {
      "task": "bbh_flan_zeroshot_word_sorting",
      "group": "bbh_flan_zeroshot",
      "dataset_path": "lukaemon/bbh",
      "dataset_name": "word_sorting",
      "test_split": "test",
      "doc_to_text": "Q: {{input}}\nA:",
      "doc_to_target": "{{target}}",
      "description": "Sort a list of words.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ],
        "do_sample": false,
        "temperature": 0.0
      },
      "repeats": 1,
      "should_decontaminate": false
    }
  },
  "versions": {
    " - bbh_flan_zeroshot_boolean_expressions": "Yaml",
    " - bbh_flan_zeroshot_causal_judgement": "Yaml",
    " - bbh_flan_zeroshot_date_understanding": "Yaml",
    " - bbh_flan_zeroshot_disambiguation_qa": "Yaml",
    " - bbh_flan_zeroshot_dyck_languages": "Yaml",
    " - bbh_flan_zeroshot_formal_fallacies": "Yaml",
    " - bbh_flan_zeroshot_geometric_shapes": "Yaml",
    " - bbh_flan_zeroshot_hyperbaton": "Yaml",
    " - bbh_flan_zeroshot_logical_deduction_five_objects": "Yaml",
    " - bbh_flan_zeroshot_logical_deduction_seven_objects": "Yaml",
    " - bbh_flan_zeroshot_logical_deduction_three_objects": "Yaml",
    " - bbh_flan_zeroshot_movie_recommendation": "Yaml",
    " - bbh_flan_zeroshot_multistep_arithmetic_two": "Yaml",
    " - bbh_flan_zeroshot_navigate": "Yaml",
    " - bbh_flan_zeroshot_object_counting": "Yaml",
    " - bbh_flan_zeroshot_penguins_in_a_table": "Yaml",
    " - bbh_flan_zeroshot_reasoning_about_colored_objects": "Yaml",
    " - bbh_flan_zeroshot_ruin_names": "Yaml",
    " - bbh_flan_zeroshot_salient_translation_error_detection": "Yaml",
    " - bbh_flan_zeroshot_snarks": "Yaml",
    " - bbh_flan_zeroshot_sports_understanding": "Yaml",
    " - bbh_flan_zeroshot_temporal_sequences": "Yaml",
    " - bbh_flan_zeroshot_tracking_shuffled_objects_five_objects": "Yaml",
    " - bbh_flan_zeroshot_tracking_shuffled_objects_seven_objects": "Yaml",
    " - bbh_flan_zeroshot_tracking_shuffled_objects_three_objects": "Yaml",
    " - bbh_flan_zeroshot_web_of_lies": "Yaml",
    " - bbh_flan_zeroshot_word_sorting": "Yaml",
    "bbh_flan_zeroshot": "N/A"
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=/fsx/lintangsutawika/00-improved-t5/checkpoints/hf/t5-v2-base-2M-flan",
    "batch_size": "4",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000
  },
  "git_hash": "b439c98"
}