{
  "results": {
    "mmlu_flan_n_shot_loglikelihood": {
      "acc,none": 0.26264064948013105,
      "acc_stderr,none": 0.04838632783976099,
      "acc_norm,none": 0.26264064948013105,
      "acc_norm_stderr,none": 0.04838632783976099
    },
    " - mmlu_flan_n_shot_loglikelihood_humanities": {
      "acc,none": 0.2684378320935175,
      "acc_stderr,none": 0.037855132822523146,
      "acc_norm,none": 0.2684378320935175,
      "acc_norm_stderr,none": 0.037855132822523146
    },
    "  - mmlu_flan_n_shot_loglikelihood_formal_logic": {
      "acc,none": 0.2222222222222222,
      "acc_stderr,none": 0.037184890068181146,
      "acc_norm,none": 0.2222222222222222,
      "acc_norm_stderr,none": 0.037184890068181146
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_european_history": {
      "acc,none": 0.2606060606060606,
      "acc_stderr,none": 0.03427743175816524,
      "acc_norm,none": 0.2606060606060606,
      "acc_norm_stderr,none": 0.03427743175816524
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_us_history": {
      "acc,none": 0.24509803921568626,
      "acc_stderr,none": 0.03019028245350194,
      "acc_norm,none": 0.24509803921568626,
      "acc_norm_stderr,none": 0.03019028245350194
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_world_history": {
      "acc,none": 0.31645569620253167,
      "acc_stderr,none": 0.030274974880218974,
      "acc_norm,none": 0.31645569620253167,
      "acc_norm_stderr,none": 0.030274974880218974
    },
    "  - mmlu_flan_n_shot_loglikelihood_international_law": {
      "acc,none": 0.256198347107438,
      "acc_stderr,none": 0.03984979653302872,
      "acc_norm,none": 0.256198347107438,
      "acc_norm_stderr,none": 0.03984979653302872
    },
    "  - mmlu_flan_n_shot_loglikelihood_jurisprudence": {
      "acc,none": 0.3333333333333333,
      "acc_stderr,none": 0.04557239513497752,
      "acc_norm,none": 0.3333333333333333,
      "acc_norm_stderr,none": 0.04557239513497752
    },
    "  - mmlu_flan_n_shot_loglikelihood_logical_fallacies": {
      "acc,none": 0.3496932515337423,
      "acc_stderr,none": 0.03746668325470023,
      "acc_norm,none": 0.3496932515337423,
      "acc_norm_stderr,none": 0.03746668325470023
    },
    "  - mmlu_flan_n_shot_loglikelihood_moral_disputes": {
      "acc,none": 0.32947976878612717,
      "acc_stderr,none": 0.02530525813187972,
      "acc_norm,none": 0.32947976878612717,
      "acc_norm_stderr,none": 0.02530525813187972
    },
    "  - mmlu_flan_n_shot_loglikelihood_moral_scenarios": {
      "acc,none": 0.23798882681564246,
      "acc_stderr,none": 0.014242630070574885,
      "acc_norm,none": 0.23798882681564246,
      "acc_norm_stderr,none": 0.014242630070574885
    },
    "  - mmlu_flan_n_shot_loglikelihood_philosophy": {
      "acc,none": 0.29260450160771706,
      "acc_stderr,none": 0.02583989833487798,
      "acc_norm,none": 0.29260450160771706,
      "acc_norm_stderr,none": 0.02583989833487798
    },
    "  - mmlu_flan_n_shot_loglikelihood_prehistory": {
      "acc,none": 0.20987654320987653,
      "acc_stderr,none": 0.022658344085981365,
      "acc_norm,none": 0.20987654320987653,
      "acc_norm_stderr,none": 0.022658344085981365
    },
    "  - mmlu_flan_n_shot_loglikelihood_professional_law": {
      "acc,none": 0.2646675358539765,
      "acc_stderr,none": 0.011267332992845526,
      "acc_norm,none": 0.2646675358539765,
      "acc_norm_stderr,none": 0.011267332992845526
    },
    "  - mmlu_flan_n_shot_loglikelihood_world_religions": {
      "acc,none": 0.2982456140350877,
      "acc_stderr,none": 0.03508771929824565,
      "acc_norm,none": 0.2982456140350877,
      "acc_norm_stderr,none": 0.03508771929824565
    },
    " - mmlu_flan_n_shot_loglikelihood_other": {
      "acc,none": 0.2912777598970068,
      "acc_stderr,none": 0.055302081907222966,
      "acc_norm,none": 0.2912777598970068,
      "acc_norm_stderr,none": 0.055302081907222966
    },
    "  - mmlu_flan_n_shot_loglikelihood_business_ethics": {
      "acc,none": 0.35,
      "acc_stderr,none": 0.0479372485441102,
      "acc_norm,none": 0.35,
      "acc_norm_stderr,none": 0.0479372485441102
    },
    "  - mmlu_flan_n_shot_loglikelihood_clinical_knowledge": {
      "acc,none": 0.24150943396226415,
      "acc_stderr,none": 0.026341480371118366,
      "acc_norm,none": 0.24150943396226415,
      "acc_norm_stderr,none": 0.026341480371118366
    },
    "  - mmlu_flan_n_shot_loglikelihood_college_medicine": {
      "acc,none": 0.2543352601156069,
      "acc_stderr,none": 0.0332055644308557,
      "acc_norm,none": 0.2543352601156069,
      "acc_norm_stderr,none": 0.0332055644308557
    },
    "  - mmlu_flan_n_shot_loglikelihood_global_facts": {
      "acc,none": 0.19,
      "acc_stderr,none": 0.03942772444036624,
      "acc_norm,none": 0.19,
      "acc_norm_stderr,none": 0.03942772444036624
    },
    "  - mmlu_flan_n_shot_loglikelihood_human_aging": {
      "acc,none": 0.3632286995515695,
      "acc_stderr,none": 0.032277904428505,
      "acc_norm,none": 0.3632286995515695,
      "acc_norm_stderr,none": 0.032277904428505
    },
    "  - mmlu_flan_n_shot_loglikelihood_management": {
      "acc,none": 0.24271844660194175,
      "acc_stderr,none": 0.04245022486384495,
      "acc_norm,none": 0.24271844660194175,
      "acc_norm_stderr,none": 0.04245022486384495
    },
    "  - mmlu_flan_n_shot_loglikelihood_marketing": {
      "acc,none": 0.41025641025641024,
      "acc_stderr,none": 0.032224140452411065,
      "acc_norm,none": 0.41025641025641024,
      "acc_norm_stderr,none": 0.032224140452411065
    },
    "  - mmlu_flan_n_shot_loglikelihood_medical_genetics": {
      "acc,none": 0.32,
      "acc_stderr,none": 0.046882617226215034,
      "acc_norm,none": 0.32,
      "acc_norm_stderr,none": 0.046882617226215034
    },
    "  - mmlu_flan_n_shot_loglikelihood_miscellaneous": {
      "acc,none": 0.29246487867177523,
      "acc_stderr,none": 0.016267000684598642,
      "acc_norm,none": 0.29246487867177523,
      "acc_norm_stderr,none": 0.016267000684598642
    },
    "  - mmlu_flan_n_shot_loglikelihood_nutrition": {
      "acc,none": 0.30718954248366015,
      "acc_stderr,none": 0.02641560191438898,
      "acc_norm,none": 0.30718954248366015,
      "acc_norm_stderr,none": 0.02641560191438898
    },
    "  - mmlu_flan_n_shot_loglikelihood_professional_accounting": {
      "acc,none": 0.2624113475177305,
      "acc_stderr,none": 0.026244920349843,
      "acc_norm,none": 0.2624113475177305,
      "acc_norm_stderr,none": 0.026244920349843
    },
    "  - mmlu_flan_n_shot_loglikelihood_professional_medicine": {
      "acc,none": 0.23161764705882354,
      "acc_stderr,none": 0.025626533803777562,
      "acc_norm,none": 0.23161764705882354,
      "acc_norm_stderr,none": 0.025626533803777562
    },
    "  - mmlu_flan_n_shot_loglikelihood_virology": {
      "acc,none": 0.29518072289156627,
      "acc_stderr,none": 0.035509201856896294,
      "acc_norm,none": 0.29518072289156627,
      "acc_norm_stderr,none": 0.035509201856896294
    },
    " - mmlu_flan_n_shot_loglikelihood_social_sciences": {
      "acc,none": 0.2505687357816055,
      "acc_stderr,none": 0.044388846813775945,
      "acc_norm,none": 0.2505687357816055,
      "acc_norm_stderr,none": 0.044388846813775945
    },
    "  - mmlu_flan_n_shot_loglikelihood_econometrics": {
      "acc,none": 0.22807017543859648,
      "acc_stderr,none": 0.03947152782669415,
      "acc_norm,none": 0.22807017543859648,
      "acc_norm_stderr,none": 0.03947152782669415
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_geography": {
      "acc,none": 0.20707070707070707,
      "acc_stderr,none": 0.028869778460267063,
      "acc_norm,none": 0.20707070707070707,
      "acc_norm_stderr,none": 0.028869778460267063
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_government_and_politics": {
      "acc,none": 0.22797927461139897,
      "acc_stderr,none": 0.030276909945178253,
      "acc_norm,none": 0.22797927461139897,
      "acc_norm_stderr,none": 0.030276909945178253
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_macroeconomics": {
      "acc,none": 0.2153846153846154,
      "acc_stderr,none": 0.020843034557462874,
      "acc_norm,none": 0.2153846153846154,
      "acc_norm_stderr,none": 0.020843034557462874
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_microeconomics": {
      "acc,none": 0.23949579831932774,
      "acc_stderr,none": 0.02772206549336127,
      "acc_norm,none": 0.23949579831932774,
      "acc_norm_stderr,none": 0.02772206549336127
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_psychology": {
      "acc,none": 0.24036697247706423,
      "acc_stderr,none": 0.01832060732096407,
      "acc_norm,none": 0.24036697247706423,
      "acc_norm_stderr,none": 0.01832060732096407
    },
    "  - mmlu_flan_n_shot_loglikelihood_human_sexuality": {
      "acc,none": 0.29770992366412213,
      "acc_stderr,none": 0.04010358942462202,
      "acc_norm,none": 0.29770992366412213,
      "acc_norm_stderr,none": 0.04010358942462202
    },
    "  - mmlu_flan_n_shot_loglikelihood_professional_psychology": {
      "acc,none": 0.272875816993464,
      "acc_stderr,none": 0.018020474148393577,
      "acc_norm,none": 0.272875816993464,
      "acc_norm_stderr,none": 0.018020474148393577
    },
    "  - mmlu_flan_n_shot_loglikelihood_public_relations": {
      "acc,none": 0.2727272727272727,
      "acc_stderr,none": 0.04265792110940588,
      "acc_norm,none": 0.2727272727272727,
      "acc_norm_stderr,none": 0.04265792110940588
    },
    "  - mmlu_flan_n_shot_loglikelihood_security_studies": {
      "acc,none": 0.19591836734693877,
      "acc_stderr,none": 0.025409301953225678,
      "acc_norm,none": 0.19591836734693877,
      "acc_norm_stderr,none": 0.025409301953225678
    },
    "  - mmlu_flan_n_shot_loglikelihood_sociology": {
      "acc,none": 0.34328358208955223,
      "acc_stderr,none": 0.03357379665433431,
      "acc_norm,none": 0.34328358208955223,
      "acc_norm_stderr,none": 0.03357379665433431
    },
    "  - mmlu_flan_n_shot_loglikelihood_us_foreign_policy": {
      "acc,none": 0.35,
      "acc_stderr,none": 0.047937248544110196,
      "acc_norm,none": 0.35,
      "acc_norm_stderr,none": 0.047937248544110196
    },
    " - mmlu_flan_n_shot_loglikelihood_stem": {
      "acc,none": 0.23755153821757058,
      "acc_stderr,none": 0.0499170274018117,
      "acc_norm,none": 0.23755153821757058,
      "acc_norm_stderr,none": 0.0499170274018117
    },
    "  - mmlu_flan_n_shot_loglikelihood_abstract_algebra": {
      "acc,none": 0.23,
      "acc_stderr,none": 0.04229525846816506,
      "acc_norm,none": 0.23,
      "acc_norm_stderr,none": 0.04229525846816506
    },
    "  - mmlu_flan_n_shot_loglikelihood_anatomy": {
      "acc,none": 0.2518518518518518,
      "acc_stderr,none": 0.03749850709174022,
      "acc_norm,none": 0.2518518518518518,
      "acc_norm_stderr,none": 0.03749850709174022
    },
    "  - mmlu_flan_n_shot_loglikelihood_astronomy": {
      "acc,none": 0.21710526315789475,
      "acc_stderr,none": 0.033550453048829226,
      "acc_norm,none": 0.21710526315789475,
      "acc_norm_stderr,none": 0.033550453048829226
    },
    "  - mmlu_flan_n_shot_loglikelihood_college_biology": {
      "acc,none": 0.2916666666666667,
      "acc_stderr,none": 0.03800968060554858,
      "acc_norm,none": 0.2916666666666667,
      "acc_norm_stderr,none": 0.03800968060554858
    },
    "  - mmlu_flan_n_shot_loglikelihood_college_chemistry": {
      "acc,none": 0.21,
      "acc_stderr,none": 0.040936018074033256,
      "acc_norm,none": 0.21,
      "acc_norm_stderr,none": 0.040936018074033256
    },
    "  - mmlu_flan_n_shot_loglikelihood_college_computer_science": {
      "acc,none": 0.31,
      "acc_stderr,none": 0.04648231987117316,
      "acc_norm,none": 0.31,
      "acc_norm_stderr,none": 0.04648231987117316
    },
    "  - mmlu_flan_n_shot_loglikelihood_college_mathematics": {
      "acc,none": 0.24,
      "acc_stderr,none": 0.042923469599092816,
      "acc_norm,none": 0.24,
      "acc_norm_stderr,none": 0.042923469599092816
    },
    "  - mmlu_flan_n_shot_loglikelihood_college_physics": {
      "acc,none": 0.22549019607843138,
      "acc_stderr,none": 0.041583075330832865,
      "acc_norm,none": 0.22549019607843138,
      "acc_norm_stderr,none": 0.041583075330832865
    },
    "  - mmlu_flan_n_shot_loglikelihood_computer_security": {
      "acc,none": 0.3,
      "acc_stderr,none": 0.046056618647183814,
      "acc_norm,none": 0.3,
      "acc_norm_stderr,none": 0.046056618647183814
    },
    "  - mmlu_flan_n_shot_loglikelihood_conceptual_physics": {
      "acc,none": 0.2851063829787234,
      "acc_stderr,none": 0.02951319662553935,
      "acc_norm,none": 0.2851063829787234,
      "acc_norm_stderr,none": 0.02951319662553935
    },
    "  - mmlu_flan_n_shot_loglikelihood_electrical_engineering": {
      "acc,none": 0.296551724137931,
      "acc_stderr,none": 0.038061426873099935,
      "acc_norm,none": 0.296551724137931,
      "acc_norm_stderr,none": 0.038061426873099935
    },
    "  - mmlu_flan_n_shot_loglikelihood_elementary_mathematics": {
      "acc,none": 0.20899470899470898,
      "acc_stderr,none": 0.020940481565334852,
      "acc_norm,none": 0.20899470899470898,
      "acc_norm_stderr,none": 0.020940481565334852
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_biology": {
      "acc,none": 0.23225806451612904,
      "acc_stderr,none": 0.02402225613030824,
      "acc_norm,none": 0.23225806451612904,
      "acc_norm_stderr,none": 0.02402225613030824
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_chemistry": {
      "acc,none": 0.15763546798029557,
      "acc_stderr,none": 0.025639014131172404,
      "acc_norm,none": 0.15763546798029557,
      "acc_norm_stderr,none": 0.025639014131172404
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_computer_science": {
      "acc,none": 0.28,
      "acc_stderr,none": 0.04512608598542127,
      "acc_norm,none": 0.28,
      "acc_norm_stderr,none": 0.04512608598542127
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_mathematics": {
      "acc,none": 0.2074074074074074,
      "acc_stderr,none": 0.024720713193952137,
      "acc_norm,none": 0.2074074074074074,
      "acc_norm_stderr,none": 0.024720713193952137
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_physics": {
      "acc,none": 0.2052980132450331,
      "acc_stderr,none": 0.03297986648473836,
      "acc_norm,none": 0.2052980132450331,
      "acc_norm_stderr,none": 0.03297986648473836
    },
    "  - mmlu_flan_n_shot_loglikelihood_high_school_statistics": {
      "acc,none": 0.2037037037037037,
      "acc_stderr,none": 0.027467401804057986,
      "acc_norm,none": 0.2037037037037037,
      "acc_norm_stderr,none": 0.027467401804057986
    },
    "  - mmlu_flan_n_shot_loglikelihood_machine_learning": {
      "acc,none": 0.32142857142857145,
      "acc_stderr,none": 0.044328040552915185,
      "acc_norm,none": 0.32142857142857145,
      "acc_norm_stderr,none": 0.044328040552915185
    }
  },
  "groups": {
    "mmlu_flan_n_shot_loglikelihood": {
      "acc,none": 0.26264064948013105,
      "acc_stderr,none": 0.04838632783976099,
      "acc_norm,none": 0.26264064948013105,
      "acc_norm_stderr,none": 0.04838632783976099
    },
    " - mmlu_flan_n_shot_loglikelihood_humanities": {
      "acc,none": 0.2684378320935175,
      "acc_stderr,none": 0.037855132822523146,
      "acc_norm,none": 0.2684378320935175,
      "acc_norm_stderr,none": 0.037855132822523146
    },
    " - mmlu_flan_n_shot_loglikelihood_other": {
      "acc,none": 0.2912777598970068,
      "acc_stderr,none": 0.055302081907222966,
      "acc_norm,none": 0.2912777598970068,
      "acc_norm_stderr,none": 0.055302081907222966
    },
    " - mmlu_flan_n_shot_loglikelihood_social_sciences": {
      "acc,none": 0.2505687357816055,
      "acc_stderr,none": 0.044388846813775945,
      "acc_norm,none": 0.2505687357816055,
      "acc_norm_stderr,none": 0.044388846813775945
    },
    " - mmlu_flan_n_shot_loglikelihood_stem": {
      "acc,none": 0.23755153821757058,
      "acc_stderr,none": 0.0499170274018117,
      "acc_norm,none": 0.23755153821757058,
      "acc_norm_stderr,none": 0.0499170274018117
    }
  },
  "configs": {
    "mmlu_flan_n_shot_loglikelihood_abstract_algebra": {
      "task": "mmlu_flan_n_shot_loglikelihood_abstract_algebra",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "abstract_algebra",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_anatomy": {
      "task": "mmlu_flan_n_shot_loglikelihood_anatomy",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "anatomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_astronomy": {
      "task": "mmlu_flan_n_shot_loglikelihood_astronomy",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "astronomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_business_ethics": {
      "task": "mmlu_flan_n_shot_loglikelihood_business_ethics",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "business_ethics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_clinical_knowledge": {
      "task": "mmlu_flan_n_shot_loglikelihood_clinical_knowledge",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "clinical_knowledge",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_college_biology": {
      "task": "mmlu_flan_n_shot_loglikelihood_college_biology",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_college_chemistry": {
      "task": "mmlu_flan_n_shot_loglikelihood_college_chemistry",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_college_computer_science": {
      "task": "mmlu_flan_n_shot_loglikelihood_college_computer_science",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_college_mathematics": {
      "task": "mmlu_flan_n_shot_loglikelihood_college_mathematics",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_college_medicine": {
      "task": "mmlu_flan_n_shot_loglikelihood_college_medicine",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_college_physics": {
      "task": "mmlu_flan_n_shot_loglikelihood_college_physics",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_computer_security": {
      "task": "mmlu_flan_n_shot_loglikelihood_computer_security",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "computer_security",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_conceptual_physics": {
      "task": "mmlu_flan_n_shot_loglikelihood_conceptual_physics",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "conceptual_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_econometrics": {
      "task": "mmlu_flan_n_shot_loglikelihood_econometrics",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "econometrics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_electrical_engineering": {
      "task": "mmlu_flan_n_shot_loglikelihood_electrical_engineering",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "electrical_engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_elementary_mathematics": {
      "task": "mmlu_flan_n_shot_loglikelihood_elementary_mathematics",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "elementary_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_formal_logic": {
      "task": "mmlu_flan_n_shot_loglikelihood_formal_logic",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "formal_logic",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_global_facts": {
      "task": "mmlu_flan_n_shot_loglikelihood_global_facts",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "global_facts",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_biology": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_biology",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_chemistry": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_chemistry",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_computer_science": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_computer_science",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_european_history": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_european_history",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_european_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_geography": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_geography",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_geography",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_government_and_politics": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_government_and_politics",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_government_and_politics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_macroeconomics": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_macroeconomics",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_macroeconomics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_mathematics": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_mathematics",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_microeconomics": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_microeconomics",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_microeconomics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_physics": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_physics",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_psychology": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_psychology",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_statistics": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_statistics",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_statistics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_us_history": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_us_history",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_us_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_high_school_world_history": {
      "task": "mmlu_flan_n_shot_loglikelihood_high_school_world_history",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_world_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_human_aging": {
      "task": "mmlu_flan_n_shot_loglikelihood_human_aging",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "human_aging",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_human_sexuality": {
      "task": "mmlu_flan_n_shot_loglikelihood_human_sexuality",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "human_sexuality",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_international_law": {
      "task": "mmlu_flan_n_shot_loglikelihood_international_law",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "international_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about international law.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_jurisprudence": {
      "task": "mmlu_flan_n_shot_loglikelihood_jurisprudence",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "jurisprudence",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_logical_fallacies": {
      "task": "mmlu_flan_n_shot_loglikelihood_logical_fallacies",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "logical_fallacies",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_machine_learning": {
      "task": "mmlu_flan_n_shot_loglikelihood_machine_learning",
      "group": "mmlu_flan_n_shot_loglikelihood_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "machine_learning",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_management": {
      "task": "mmlu_flan_n_shot_loglikelihood_management",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about management.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_marketing": {
      "task": "mmlu_flan_n_shot_loglikelihood_marketing",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "marketing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_medical_genetics": {
      "task": "mmlu_flan_n_shot_loglikelihood_medical_genetics",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "medical_genetics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_miscellaneous": {
      "task": "mmlu_flan_n_shot_loglikelihood_miscellaneous",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "miscellaneous",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_moral_disputes": {
      "task": "mmlu_flan_n_shot_loglikelihood_moral_disputes",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "moral_disputes",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_moral_scenarios": {
      "task": "mmlu_flan_n_shot_loglikelihood_moral_scenarios",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "moral_scenarios",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_nutrition": {
      "task": "mmlu_flan_n_shot_loglikelihood_nutrition",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "nutrition",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_philosophy": {
      "task": "mmlu_flan_n_shot_loglikelihood_philosophy",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "philosophy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_prehistory": {
      "task": "mmlu_flan_n_shot_loglikelihood_prehistory",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "prehistory",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_professional_accounting": {
      "task": "mmlu_flan_n_shot_loglikelihood_professional_accounting",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_accounting",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_professional_law": {
      "task": "mmlu_flan_n_shot_loglikelihood_professional_law",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_professional_medicine": {
      "task": "mmlu_flan_n_shot_loglikelihood_professional_medicine",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_professional_psychology": {
      "task": "mmlu_flan_n_shot_loglikelihood_professional_psychology",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_public_relations": {
      "task": "mmlu_flan_n_shot_loglikelihood_public_relations",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "public_relations",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_security_studies": {
      "task": "mmlu_flan_n_shot_loglikelihood_security_studies",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "security_studies",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_sociology": {
      "task": "mmlu_flan_n_shot_loglikelihood_sociology",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "sociology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_us_foreign_policy": {
      "task": "mmlu_flan_n_shot_loglikelihood_us_foreign_policy",
      "group": "mmlu_flan_n_shot_loglikelihood_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "us_foreign_policy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_virology": {
      "task": "mmlu_flan_n_shot_loglikelihood_virology",
      "group": "mmlu_flan_n_shot_loglikelihood_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "virology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about virology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_loglikelihood_world_religions": {
      "task": "mmlu_flan_n_shot_loglikelihood_world_religions",
      "group": "mmlu_flan_n_shot_loglikelihood_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "world_religions",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "answer",
      "doc_to_choice": [
        "(A)",
        "(B)",
        "(C)",
        "(D)"
      ],
      "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "acc",
          "aggregation": "mean",
          "higher_is_better": true
        },
        {
          "metric": "acc_norm",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "multiple_choice",
      "repeats": 1,
      "should_decontaminate": false
    }
  },
  "versions": {
    "  - mmlu_flan_n_shot_loglikelihood_abstract_algebra": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_anatomy": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_astronomy": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_business_ethics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_clinical_knowledge": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_college_biology": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_college_chemistry": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_college_computer_science": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_college_mathematics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_college_medicine": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_college_physics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_computer_security": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_conceptual_physics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_econometrics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_electrical_engineering": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_elementary_mathematics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_formal_logic": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_global_facts": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_biology": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_chemistry": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_computer_science": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_european_history": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_geography": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_government_and_politics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_macroeconomics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_mathematics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_microeconomics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_physics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_psychology": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_statistics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_us_history": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_high_school_world_history": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_human_aging": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_human_sexuality": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_international_law": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_jurisprudence": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_logical_fallacies": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_machine_learning": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_management": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_marketing": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_medical_genetics": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_miscellaneous": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_moral_disputes": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_moral_scenarios": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_nutrition": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_philosophy": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_prehistory": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_professional_accounting": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_professional_law": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_professional_medicine": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_professional_psychology": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_public_relations": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_security_studies": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_sociology": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_us_foreign_policy": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_virology": "Yaml",
    "  - mmlu_flan_n_shot_loglikelihood_world_religions": "Yaml",
    " - mmlu_flan_n_shot_loglikelihood_humanities": "N/A",
    " - mmlu_flan_n_shot_loglikelihood_other": "N/A",
    " - mmlu_flan_n_shot_loglikelihood_social_sciences": "N/A",
    " - mmlu_flan_n_shot_loglikelihood_stem": "N/A",
    "mmlu_flan_n_shot_loglikelihood": "N/A"
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=/fsx/lintangsutawika/00-improved-t5/checkpoints/hf/t5-v2-base-2M-flan",
    "batch_size": "4",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000
  },
  "git_hash": "b439c98"
}