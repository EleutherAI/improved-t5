{
  "results": {
    "mmlu_flan_n_shot_generative": {
      "exact_match,none": 0.17312348668280872,
      "exact_match_stderr,none": 0.07600567740917964,
      "alias": "mmlu_flan_n_shot_generative"
    },
    "mmlu_flan_n_shot_generative_humanities": {
      "exact_match,none": 0.21402763018065887,
      "exact_match_stderr,none": 0.09258470480393792,
      "alias": " - mmlu_flan_n_shot_generative_humanities"
    },
    "mmlu_flan_n_shot_generative_formal_logic": {
      "exact_match,none": 0.12698412698412698,
      "exact_match_stderr,none": 0.029780417522688438,
      "alias": "  - mmlu_flan_n_shot_generative_formal_logic"
    },
    "mmlu_flan_n_shot_generative_high_school_european_history": {
      "exact_match,none": 0.30303030303030304,
      "exact_match_stderr,none": 0.03588624800091707,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_european_history"
    },
    "mmlu_flan_n_shot_generative_high_school_us_history": {
      "exact_match,none": 0.37745098039215685,
      "exact_match_stderr,none": 0.03402272044340703,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_us_history"
    },
    "mmlu_flan_n_shot_generative_high_school_world_history": {
      "exact_match,none": 0.3037974683544304,
      "exact_match_stderr,none": 0.029936696387138615,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_world_history"
    },
    "mmlu_flan_n_shot_generative_international_law": {
      "exact_match,none": 0.3140495867768595,
      "exact_match_stderr,none": 0.04236964753041018,
      "alias": "  - mmlu_flan_n_shot_generative_international_law"
    },
    "mmlu_flan_n_shot_generative_jurisprudence": {
      "exact_match,none": 0.2222222222222222,
      "exact_match_stderr,none": 0.040191074725573483,
      "alias": "  - mmlu_flan_n_shot_generative_jurisprudence"
    },
    "mmlu_flan_n_shot_generative_logical_fallacies": {
      "exact_match,none": 0.08588957055214724,
      "exact_match_stderr,none": 0.022014662933817524,
      "alias": "  - mmlu_flan_n_shot_generative_logical_fallacies"
    },
    "mmlu_flan_n_shot_generative_moral_disputes": {
      "exact_match,none": 0.26878612716763006,
      "exact_match_stderr,none": 0.023868003262500104,
      "alias": "  - mmlu_flan_n_shot_generative_moral_disputes"
    },
    "mmlu_flan_n_shot_generative_moral_scenarios": {
      "exact_match,none": 0.01005586592178771,
      "exact_match_stderr,none": 0.0033369240418078196,
      "alias": "  - mmlu_flan_n_shot_generative_moral_scenarios"
    },
    "mmlu_flan_n_shot_generative_philosophy": {
      "exact_match,none": 0.22508038585209003,
      "exact_match_stderr,none": 0.023720088516179034,
      "alias": "  - mmlu_flan_n_shot_generative_philosophy"
    },
    "mmlu_flan_n_shot_generative_prehistory": {
      "exact_match,none": 0.16049382716049382,
      "exact_match_stderr,none": 0.020423955354778027,
      "alias": "  - mmlu_flan_n_shot_generative_prehistory"
    },
    "mmlu_flan_n_shot_generative_professional_law": {
      "exact_match,none": 0.3089960886571056,
      "exact_match_stderr,none": 0.011801729777239242,
      "alias": "  - mmlu_flan_n_shot_generative_professional_law"
    },
    "mmlu_flan_n_shot_generative_world_religions": {
      "exact_match,none": 0.10526315789473684,
      "exact_match_stderr,none": 0.023537557657892557,
      "alias": "  - mmlu_flan_n_shot_generative_world_religions"
    },
    "mmlu_flan_n_shot_generative_other": {
      "exact_match,none": 0.15159317669777922,
      "exact_match_stderr,none": 0.07803266810476035,
      "alias": " - mmlu_flan_n_shot_generative_other"
    },
    "mmlu_flan_n_shot_generative_business_ethics": {
      "exact_match,none": 0.22,
      "exact_match_stderr,none": 0.04163331998932269,
      "alias": "  - mmlu_flan_n_shot_generative_business_ethics"
    },
    "mmlu_flan_n_shot_generative_clinical_knowledge": {
      "exact_match,none": 0.30943396226415093,
      "exact_match_stderr,none": 0.028450154794118627,
      "alias": "  - mmlu_flan_n_shot_generative_clinical_knowledge"
    },
    "mmlu_flan_n_shot_generative_college_medicine": {
      "exact_match,none": 0.2543352601156069,
      "exact_match_stderr,none": 0.0332055644308557,
      "alias": "  - mmlu_flan_n_shot_generative_college_medicine"
    },
    "mmlu_flan_n_shot_generative_global_facts": {
      "exact_match,none": 0.17,
      "exact_match_stderr,none": 0.03775251680686371,
      "alias": "  - mmlu_flan_n_shot_generative_global_facts"
    },
    "mmlu_flan_n_shot_generative_human_aging": {
      "exact_match,none": 0.013452914798206279,
      "exact_match_stderr,none": 0.007731978139734137,
      "alias": "  - mmlu_flan_n_shot_generative_human_aging"
    },
    "mmlu_flan_n_shot_generative_management": {
      "exact_match,none": 0.1650485436893204,
      "exact_match_stderr,none": 0.03675668832233189,
      "alias": "  - mmlu_flan_n_shot_generative_management"
    },
    "mmlu_flan_n_shot_generative_marketing": {
      "exact_match,none": 0.21794871794871795,
      "exact_match_stderr,none": 0.02704685763071666,
      "alias": "  - mmlu_flan_n_shot_generative_marketing"
    },
    "mmlu_flan_n_shot_generative_medical_genetics": {
      "exact_match,none": 0.22,
      "exact_match_stderr,none": 0.0416333199893227,
      "alias": "  - mmlu_flan_n_shot_generative_medical_genetics"
    },
    "mmlu_flan_n_shot_generative_miscellaneous": {
      "exact_match,none": 0.08812260536398467,
      "exact_match_stderr,none": 0.010136978203312634,
      "alias": "  - mmlu_flan_n_shot_generative_miscellaneous"
    },
    "mmlu_flan_n_shot_generative_nutrition": {
      "exact_match,none": 0.1568627450980392,
      "exact_match_stderr,none": 0.020823758837580905,
      "alias": "  - mmlu_flan_n_shot_generative_nutrition"
    },
    "mmlu_flan_n_shot_generative_professional_accounting": {
      "exact_match,none": 0.2624113475177305,
      "exact_match_stderr,none": 0.026244920349843007,
      "alias": "  - mmlu_flan_n_shot_generative_professional_accounting"
    },
    "mmlu_flan_n_shot_generative_professional_medicine": {
      "exact_match,none": 0.025735294117647058,
      "exact_match_stderr,none": 0.009618744913240848,
      "alias": "  - mmlu_flan_n_shot_generative_professional_medicine"
    },
    "mmlu_flan_n_shot_generative_virology": {
      "exact_match,none": 0.09036144578313253,
      "exact_match_stderr,none": 0.02231947850198775,
      "alias": "  - mmlu_flan_n_shot_generative_virology"
    },
    "mmlu_flan_n_shot_generative_social_sciences": {
      "exact_match,none": 0.1387715307117322,
      "exact_match_stderr,none": 0.05184506943448477,
      "alias": " - mmlu_flan_n_shot_generative_social_sciences"
    },
    "mmlu_flan_n_shot_generative_econometrics": {
      "exact_match,none": 0.13157894736842105,
      "exact_match_stderr,none": 0.03179941669997663,
      "alias": "  - mmlu_flan_n_shot_generative_econometrics"
    },
    "mmlu_flan_n_shot_generative_high_school_geography": {
      "exact_match,none": 0.13131313131313133,
      "exact_match_stderr,none": 0.02406315641682252,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_geography"
    },
    "mmlu_flan_n_shot_generative_high_school_government_and_politics": {
      "exact_match,none": 0.09844559585492228,
      "exact_match_stderr,none": 0.021500249576033467,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_government_and_politics"
    },
    "mmlu_flan_n_shot_generative_high_school_macroeconomics": {
      "exact_match,none": 0.17435897435897435,
      "exact_match_stderr,none": 0.01923724980340523,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_macroeconomics"
    },
    "mmlu_flan_n_shot_generative_high_school_microeconomics": {
      "exact_match,none": 0.15966386554621848,
      "exact_match_stderr,none": 0.023793353997528802,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_microeconomics"
    },
    "mmlu_flan_n_shot_generative_high_school_psychology": {
      "exact_match,none": 0.05688073394495413,
      "exact_match_stderr,none": 0.009930393412586721,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_psychology"
    },
    "mmlu_flan_n_shot_generative_human_sexuality": {
      "exact_match,none": 0.06870229007633588,
      "exact_match_stderr,none": 0.022184936922745042,
      "alias": "  - mmlu_flan_n_shot_generative_human_sexuality"
    },
    "mmlu_flan_n_shot_generative_professional_psychology": {
      "exact_match,none": 0.15849673202614378,
      "exact_match_stderr,none": 0.014774658600611041,
      "alias": "  - mmlu_flan_n_shot_generative_professional_psychology"
    },
    "mmlu_flan_n_shot_generative_public_relations": {
      "exact_match,none": 0.12727272727272726,
      "exact_match_stderr,none": 0.031922265124685704,
      "alias": "  - mmlu_flan_n_shot_generative_public_relations"
    },
    "mmlu_flan_n_shot_generative_security_studies": {
      "exact_match,none": 0.2612244897959184,
      "exact_match_stderr,none": 0.02812342933514279,
      "alias": "  - mmlu_flan_n_shot_generative_security_studies"
    },
    "mmlu_flan_n_shot_generative_sociology": {
      "exact_match,none": 0.15920398009950248,
      "exact_match_stderr,none": 0.02587064676616914,
      "alias": "  - mmlu_flan_n_shot_generative_sociology"
    },
    "mmlu_flan_n_shot_generative_us_foreign_policy": {
      "exact_match,none": 0.14,
      "exact_match_stderr,none": 0.03487350880197772,
      "alias": "  - mmlu_flan_n_shot_generative_us_foreign_policy"
    },
    "mmlu_flan_n_shot_generative_stem": {
      "exact_match,none": 0.16682524579765298,
      "exact_match_stderr,none": 0.050309885805027474,
      "alias": " - mmlu_flan_n_shot_generative_stem"
    },
    "mmlu_flan_n_shot_generative_abstract_algebra": {
      "exact_match,none": 0.19,
      "exact_match_stderr,none": 0.03942772444036622,
      "alias": "  - mmlu_flan_n_shot_generative_abstract_algebra"
    },
    "mmlu_flan_n_shot_generative_anatomy": {
      "exact_match,none": 0.1925925925925926,
      "exact_match_stderr,none": 0.03406542058502652,
      "alias": "  - mmlu_flan_n_shot_generative_anatomy"
    },
    "mmlu_flan_n_shot_generative_astronomy": {
      "exact_match,none": 0.11842105263157894,
      "exact_match_stderr,none": 0.026293995855474924,
      "alias": "  - mmlu_flan_n_shot_generative_astronomy"
    },
    "mmlu_flan_n_shot_generative_college_biology": {
      "exact_match,none": 0.11805555555555555,
      "exact_match_stderr,none": 0.026983346503309347,
      "alias": "  - mmlu_flan_n_shot_generative_college_biology"
    },
    "mmlu_flan_n_shot_generative_college_chemistry": {
      "exact_match,none": 0.22,
      "exact_match_stderr,none": 0.04163331998932269,
      "alias": "  - mmlu_flan_n_shot_generative_college_chemistry"
    },
    "mmlu_flan_n_shot_generative_college_computer_science": {
      "exact_match,none": 0.19,
      "exact_match_stderr,none": 0.039427724440366234,
      "alias": "  - mmlu_flan_n_shot_generative_college_computer_science"
    },
    "mmlu_flan_n_shot_generative_college_mathematics": {
      "exact_match,none": 0.14,
      "exact_match_stderr,none": 0.034873508801977704,
      "alias": "  - mmlu_flan_n_shot_generative_college_mathematics"
    },
    "mmlu_flan_n_shot_generative_college_physics": {
      "exact_match,none": 0.11764705882352941,
      "exact_match_stderr,none": 0.03205907733144526,
      "alias": "  - mmlu_flan_n_shot_generative_college_physics"
    },
    "mmlu_flan_n_shot_generative_computer_security": {
      "exact_match,none": 0.23,
      "exact_match_stderr,none": 0.04229525846816506,
      "alias": "  - mmlu_flan_n_shot_generative_computer_security"
    },
    "mmlu_flan_n_shot_generative_conceptual_physics": {
      "exact_match,none": 0.0851063829787234,
      "exact_match_stderr,none": 0.01824141134457229,
      "alias": "  - mmlu_flan_n_shot_generative_conceptual_physics"
    },
    "mmlu_flan_n_shot_generative_electrical_engineering": {
      "exact_match,none": 0.2413793103448276,
      "exact_match_stderr,none": 0.03565998174135303,
      "alias": "  - mmlu_flan_n_shot_generative_electrical_engineering"
    },
    "mmlu_flan_n_shot_generative_elementary_mathematics": {
      "exact_match,none": 0.21164021164021163,
      "exact_match_stderr,none": 0.02103733150526289,
      "alias": "  - mmlu_flan_n_shot_generative_elementary_mathematics"
    },
    "mmlu_flan_n_shot_generative_high_school_biology": {
      "exact_match,none": 0.14193548387096774,
      "exact_match_stderr,none": 0.019853003676559754,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_biology"
    },
    "mmlu_flan_n_shot_generative_high_school_chemistry": {
      "exact_match,none": 0.12315270935960591,
      "exact_match_stderr,none": 0.0231210888624031,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_chemistry"
    },
    "mmlu_flan_n_shot_generative_high_school_computer_science": {
      "exact_match,none": 0.21,
      "exact_match_stderr,none": 0.04093601807403326,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_computer_science"
    },
    "mmlu_flan_n_shot_generative_high_school_mathematics": {
      "exact_match,none": 0.2,
      "exact_match_stderr,none": 0.02438843043398766,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_mathematics"
    },
    "mmlu_flan_n_shot_generative_high_school_physics": {
      "exact_match,none": 0.10596026490066225,
      "exact_match_stderr,none": 0.02513068339206657,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_physics"
    },
    "mmlu_flan_n_shot_generative_high_school_statistics": {
      "exact_match,none": 0.16666666666666666,
      "exact_match_stderr,none": 0.02541642838876748,
      "alias": "  - mmlu_flan_n_shot_generative_high_school_statistics"
    },
    "mmlu_flan_n_shot_generative_machine_learning": {
      "exact_match,none": 0.22321428571428573,
      "exact_match_stderr,none": 0.03952301967702511,
      "alias": "  - mmlu_flan_n_shot_generative_machine_learning"
    }
  },
  "groups": {
    "mmlu_flan_n_shot_generative": {
      "exact_match,none": 0.17312348668280872,
      "exact_match_stderr,none": 0.07600567740917964,
      "alias": "mmlu_flan_n_shot_generative"
    },
    "mmlu_flan_n_shot_generative_humanities": {
      "exact_match,none": 0.21402763018065887,
      "exact_match_stderr,none": 0.09258470480393792,
      "alias": " - mmlu_flan_n_shot_generative_humanities"
    },
    "mmlu_flan_n_shot_generative_other": {
      "exact_match,none": 0.15159317669777922,
      "exact_match_stderr,none": 0.07803266810476035,
      "alias": " - mmlu_flan_n_shot_generative_other"
    },
    "mmlu_flan_n_shot_generative_social_sciences": {
      "exact_match,none": 0.1387715307117322,
      "exact_match_stderr,none": 0.05184506943448477,
      "alias": " - mmlu_flan_n_shot_generative_social_sciences"
    },
    "mmlu_flan_n_shot_generative_stem": {
      "exact_match,none": 0.16682524579765298,
      "exact_match_stderr,none": 0.050309885805027474,
      "alias": " - mmlu_flan_n_shot_generative_stem"
    }
  },
  "configs": {
    "mmlu_flan_n_shot_generative_abstract_algebra": {
      "task": "mmlu_flan_n_shot_generative_abstract_algebra",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "abstract_algebra",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_anatomy": {
      "task": "mmlu_flan_n_shot_generative_anatomy",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "anatomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_astronomy": {
      "task": "mmlu_flan_n_shot_generative_astronomy",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "astronomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_business_ethics": {
      "task": "mmlu_flan_n_shot_generative_business_ethics",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "business_ethics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_clinical_knowledge": {
      "task": "mmlu_flan_n_shot_generative_clinical_knowledge",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "clinical_knowledge",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_college_biology": {
      "task": "mmlu_flan_n_shot_generative_college_biology",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_college_chemistry": {
      "task": "mmlu_flan_n_shot_generative_college_chemistry",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_college_computer_science": {
      "task": "mmlu_flan_n_shot_generative_college_computer_science",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_college_mathematics": {
      "task": "mmlu_flan_n_shot_generative_college_mathematics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_college_medicine": {
      "task": "mmlu_flan_n_shot_generative_college_medicine",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_college_physics": {
      "task": "mmlu_flan_n_shot_generative_college_physics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_computer_security": {
      "task": "mmlu_flan_n_shot_generative_computer_security",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "computer_security",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_conceptual_physics": {
      "task": "mmlu_flan_n_shot_generative_conceptual_physics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "conceptual_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_econometrics": {
      "task": "mmlu_flan_n_shot_generative_econometrics",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "econometrics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_electrical_engineering": {
      "task": "mmlu_flan_n_shot_generative_electrical_engineering",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "electrical_engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_elementary_mathematics": {
      "task": "mmlu_flan_n_shot_generative_elementary_mathematics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "elementary_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_formal_logic": {
      "task": "mmlu_flan_n_shot_generative_formal_logic",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "formal_logic",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_global_facts": {
      "task": "mmlu_flan_n_shot_generative_global_facts",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "global_facts",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_biology": {
      "task": "mmlu_flan_n_shot_generative_high_school_biology",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_chemistry": {
      "task": "mmlu_flan_n_shot_generative_high_school_chemistry",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_computer_science": {
      "task": "mmlu_flan_n_shot_generative_high_school_computer_science",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_european_history": {
      "task": "mmlu_flan_n_shot_generative_high_school_european_history",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_european_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_geography": {
      "task": "mmlu_flan_n_shot_generative_high_school_geography",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_geography",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_government_and_politics": {
      "task": "mmlu_flan_n_shot_generative_high_school_government_and_politics",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_government_and_politics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_macroeconomics": {
      "task": "mmlu_flan_n_shot_generative_high_school_macroeconomics",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_macroeconomics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_mathematics": {
      "task": "mmlu_flan_n_shot_generative_high_school_mathematics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_microeconomics": {
      "task": "mmlu_flan_n_shot_generative_high_school_microeconomics",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_microeconomics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_physics": {
      "task": "mmlu_flan_n_shot_generative_high_school_physics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_psychology": {
      "task": "mmlu_flan_n_shot_generative_high_school_psychology",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_statistics": {
      "task": "mmlu_flan_n_shot_generative_high_school_statistics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_statistics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_us_history": {
      "task": "mmlu_flan_n_shot_generative_high_school_us_history",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_us_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_high_school_world_history": {
      "task": "mmlu_flan_n_shot_generative_high_school_world_history",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_world_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_human_aging": {
      "task": "mmlu_flan_n_shot_generative_human_aging",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "human_aging",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_human_sexuality": {
      "task": "mmlu_flan_n_shot_generative_human_sexuality",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "human_sexuality",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_international_law": {
      "task": "mmlu_flan_n_shot_generative_international_law",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "international_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about international law.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_jurisprudence": {
      "task": "mmlu_flan_n_shot_generative_jurisprudence",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "jurisprudence",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_logical_fallacies": {
      "task": "mmlu_flan_n_shot_generative_logical_fallacies",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "logical_fallacies",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_machine_learning": {
      "task": "mmlu_flan_n_shot_generative_machine_learning",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "machine_learning",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_management": {
      "task": "mmlu_flan_n_shot_generative_management",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about management.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_marketing": {
      "task": "mmlu_flan_n_shot_generative_marketing",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "marketing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_medical_genetics": {
      "task": "mmlu_flan_n_shot_generative_medical_genetics",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "medical_genetics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_miscellaneous": {
      "task": "mmlu_flan_n_shot_generative_miscellaneous",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "miscellaneous",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_moral_disputes": {
      "task": "mmlu_flan_n_shot_generative_moral_disputes",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "moral_disputes",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_moral_scenarios": {
      "task": "mmlu_flan_n_shot_generative_moral_scenarios",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "moral_scenarios",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_nutrition": {
      "task": "mmlu_flan_n_shot_generative_nutrition",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "nutrition",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_philosophy": {
      "task": "mmlu_flan_n_shot_generative_philosophy",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "philosophy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_prehistory": {
      "task": "mmlu_flan_n_shot_generative_prehistory",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "prehistory",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_professional_accounting": {
      "task": "mmlu_flan_n_shot_generative_professional_accounting",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_accounting",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_professional_law": {
      "task": "mmlu_flan_n_shot_generative_professional_law",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_professional_medicine": {
      "task": "mmlu_flan_n_shot_generative_professional_medicine",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_professional_psychology": {
      "task": "mmlu_flan_n_shot_generative_professional_psychology",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_public_relations": {
      "task": "mmlu_flan_n_shot_generative_public_relations",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "public_relations",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_security_studies": {
      "task": "mmlu_flan_n_shot_generative_security_studies",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "security_studies",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_sociology": {
      "task": "mmlu_flan_n_shot_generative_sociology",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "sociology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_us_foreign_policy": {
      "task": "mmlu_flan_n_shot_generative_us_foreign_policy",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "us_foreign_policy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_virology": {
      "task": "mmlu_flan_n_shot_generative_virology",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "virology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about virology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    },
    "mmlu_flan_n_shot_generative_world_religions": {
      "task": "mmlu_flan_n_shot_generative_world_religions",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "world_religions",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 0,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false,
      "metadata": {
        "version": 0.0
      }
    }
  },
  "versions": {
    "mmlu_flan_n_shot_generative": "N/A",
    "mmlu_flan_n_shot_generative_abstract_algebra": 0.0,
    "mmlu_flan_n_shot_generative_anatomy": 0.0,
    "mmlu_flan_n_shot_generative_astronomy": 0.0,
    "mmlu_flan_n_shot_generative_business_ethics": 0.0,
    "mmlu_flan_n_shot_generative_clinical_knowledge": 0.0,
    "mmlu_flan_n_shot_generative_college_biology": 0.0,
    "mmlu_flan_n_shot_generative_college_chemistry": 0.0,
    "mmlu_flan_n_shot_generative_college_computer_science": 0.0,
    "mmlu_flan_n_shot_generative_college_mathematics": 0.0,
    "mmlu_flan_n_shot_generative_college_medicine": 0.0,
    "mmlu_flan_n_shot_generative_college_physics": 0.0,
    "mmlu_flan_n_shot_generative_computer_security": 0.0,
    "mmlu_flan_n_shot_generative_conceptual_physics": 0.0,
    "mmlu_flan_n_shot_generative_econometrics": 0.0,
    "mmlu_flan_n_shot_generative_electrical_engineering": 0.0,
    "mmlu_flan_n_shot_generative_elementary_mathematics": 0.0,
    "mmlu_flan_n_shot_generative_formal_logic": 0.0,
    "mmlu_flan_n_shot_generative_global_facts": 0.0,
    "mmlu_flan_n_shot_generative_high_school_biology": 0.0,
    "mmlu_flan_n_shot_generative_high_school_chemistry": 0.0,
    "mmlu_flan_n_shot_generative_high_school_computer_science": 0.0,
    "mmlu_flan_n_shot_generative_high_school_european_history": 0.0,
    "mmlu_flan_n_shot_generative_high_school_geography": 0.0,
    "mmlu_flan_n_shot_generative_high_school_government_and_politics": 0.0,
    "mmlu_flan_n_shot_generative_high_school_macroeconomics": 0.0,
    "mmlu_flan_n_shot_generative_high_school_mathematics": 0.0,
    "mmlu_flan_n_shot_generative_high_school_microeconomics": 0.0,
    "mmlu_flan_n_shot_generative_high_school_physics": 0.0,
    "mmlu_flan_n_shot_generative_high_school_psychology": 0.0,
    "mmlu_flan_n_shot_generative_high_school_statistics": 0.0,
    "mmlu_flan_n_shot_generative_high_school_us_history": 0.0,
    "mmlu_flan_n_shot_generative_high_school_world_history": 0.0,
    "mmlu_flan_n_shot_generative_human_aging": 0.0,
    "mmlu_flan_n_shot_generative_human_sexuality": 0.0,
    "mmlu_flan_n_shot_generative_humanities": "N/A",
    "mmlu_flan_n_shot_generative_international_law": 0.0,
    "mmlu_flan_n_shot_generative_jurisprudence": 0.0,
    "mmlu_flan_n_shot_generative_logical_fallacies": 0.0,
    "mmlu_flan_n_shot_generative_machine_learning": 0.0,
    "mmlu_flan_n_shot_generative_management": 0.0,
    "mmlu_flan_n_shot_generative_marketing": 0.0,
    "mmlu_flan_n_shot_generative_medical_genetics": 0.0,
    "mmlu_flan_n_shot_generative_miscellaneous": 0.0,
    "mmlu_flan_n_shot_generative_moral_disputes": 0.0,
    "mmlu_flan_n_shot_generative_moral_scenarios": 0.0,
    "mmlu_flan_n_shot_generative_nutrition": 0.0,
    "mmlu_flan_n_shot_generative_other": "N/A",
    "mmlu_flan_n_shot_generative_philosophy": 0.0,
    "mmlu_flan_n_shot_generative_prehistory": 0.0,
    "mmlu_flan_n_shot_generative_professional_accounting": 0.0,
    "mmlu_flan_n_shot_generative_professional_law": 0.0,
    "mmlu_flan_n_shot_generative_professional_medicine": 0.0,
    "mmlu_flan_n_shot_generative_professional_psychology": 0.0,
    "mmlu_flan_n_shot_generative_public_relations": 0.0,
    "mmlu_flan_n_shot_generative_security_studies": 0.0,
    "mmlu_flan_n_shot_generative_social_sciences": "N/A",
    "mmlu_flan_n_shot_generative_sociology": 0.0,
    "mmlu_flan_n_shot_generative_stem": "N/A",
    "mmlu_flan_n_shot_generative_us_foreign_policy": 0.0,
    "mmlu_flan_n_shot_generative_virology": 0.0,
    "mmlu_flan_n_shot_generative_world_religions": 0.0
  },
  "n-shot": {
    "mmlu_flan_n_shot_generative": 0,
    "mmlu_flan_n_shot_generative_abstract_algebra": 0,
    "mmlu_flan_n_shot_generative_anatomy": 0,
    "mmlu_flan_n_shot_generative_astronomy": 0,
    "mmlu_flan_n_shot_generative_business_ethics": 0,
    "mmlu_flan_n_shot_generative_clinical_knowledge": 0,
    "mmlu_flan_n_shot_generative_college_biology": 0,
    "mmlu_flan_n_shot_generative_college_chemistry": 0,
    "mmlu_flan_n_shot_generative_college_computer_science": 0,
    "mmlu_flan_n_shot_generative_college_mathematics": 0,
    "mmlu_flan_n_shot_generative_college_medicine": 0,
    "mmlu_flan_n_shot_generative_college_physics": 0,
    "mmlu_flan_n_shot_generative_computer_security": 0,
    "mmlu_flan_n_shot_generative_conceptual_physics": 0,
    "mmlu_flan_n_shot_generative_econometrics": 0,
    "mmlu_flan_n_shot_generative_electrical_engineering": 0,
    "mmlu_flan_n_shot_generative_elementary_mathematics": 0,
    "mmlu_flan_n_shot_generative_formal_logic": 0,
    "mmlu_flan_n_shot_generative_global_facts": 0,
    "mmlu_flan_n_shot_generative_high_school_biology": 0,
    "mmlu_flan_n_shot_generative_high_school_chemistry": 0,
    "mmlu_flan_n_shot_generative_high_school_computer_science": 0,
    "mmlu_flan_n_shot_generative_high_school_european_history": 0,
    "mmlu_flan_n_shot_generative_high_school_geography": 0,
    "mmlu_flan_n_shot_generative_high_school_government_and_politics": 0,
    "mmlu_flan_n_shot_generative_high_school_macroeconomics": 0,
    "mmlu_flan_n_shot_generative_high_school_mathematics": 0,
    "mmlu_flan_n_shot_generative_high_school_microeconomics": 0,
    "mmlu_flan_n_shot_generative_high_school_physics": 0,
    "mmlu_flan_n_shot_generative_high_school_psychology": 0,
    "mmlu_flan_n_shot_generative_high_school_statistics": 0,
    "mmlu_flan_n_shot_generative_high_school_us_history": 0,
    "mmlu_flan_n_shot_generative_high_school_world_history": 0,
    "mmlu_flan_n_shot_generative_human_aging": 0,
    "mmlu_flan_n_shot_generative_human_sexuality": 0,
    "mmlu_flan_n_shot_generative_humanities": 0,
    "mmlu_flan_n_shot_generative_international_law": 0,
    "mmlu_flan_n_shot_generative_jurisprudence": 0,
    "mmlu_flan_n_shot_generative_logical_fallacies": 0,
    "mmlu_flan_n_shot_generative_machine_learning": 0,
    "mmlu_flan_n_shot_generative_management": 0,
    "mmlu_flan_n_shot_generative_marketing": 0,
    "mmlu_flan_n_shot_generative_medical_genetics": 0,
    "mmlu_flan_n_shot_generative_miscellaneous": 0,
    "mmlu_flan_n_shot_generative_moral_disputes": 0,
    "mmlu_flan_n_shot_generative_moral_scenarios": 0,
    "mmlu_flan_n_shot_generative_nutrition": 0,
    "mmlu_flan_n_shot_generative_other": 0,
    "mmlu_flan_n_shot_generative_philosophy": 0,
    "mmlu_flan_n_shot_generative_prehistory": 0,
    "mmlu_flan_n_shot_generative_professional_accounting": 0,
    "mmlu_flan_n_shot_generative_professional_law": 0,
    "mmlu_flan_n_shot_generative_professional_medicine": 0,
    "mmlu_flan_n_shot_generative_professional_psychology": 0,
    "mmlu_flan_n_shot_generative_public_relations": 0,
    "mmlu_flan_n_shot_generative_security_studies": 0,
    "mmlu_flan_n_shot_generative_social_sciences": 0,
    "mmlu_flan_n_shot_generative_sociology": 0,
    "mmlu_flan_n_shot_generative_stem": 0,
    "mmlu_flan_n_shot_generative_us_foreign_policy": 0,
    "mmlu_flan_n_shot_generative_virology": 0,
    "mmlu_flan_n_shot_generative_world_religions": 0
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=lintang/t5-v2-xxl-flan,parallelize=True",
    "batch_size": "1",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000,
    "gen_kwargs": null
  },
  "git_hash": "c12d8b7"
}