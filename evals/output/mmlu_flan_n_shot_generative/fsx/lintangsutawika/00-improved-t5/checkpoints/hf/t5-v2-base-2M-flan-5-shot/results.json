{
  "results": {
    "mmlu_flan_n_shot_generative": {
      "exact_match,none": 0.24405355362483977,
      "exact_match_stderr,none": 0.10104199502995338
    },
    " - mmlu_flan_n_shot_generative_humanities": {
      "exact_match,none": 0.2724760892667375,
      "exact_match_stderr,none": 0.07700331605831495
    },
    "  - mmlu_flan_n_shot_generative_formal_logic": {
      "exact_match,none": 0.29365079365079366,
      "exact_match_stderr,none": 0.04073524322147126
    },
    "  - mmlu_flan_n_shot_generative_high_school_european_history": {
      "exact_match,none": 0.38181818181818183,
      "exact_match_stderr,none": 0.03793713171165634
    },
    "  - mmlu_flan_n_shot_generative_high_school_us_history": {
      "exact_match,none": 0.35784313725490197,
      "exact_match_stderr,none": 0.03364487286088299
    },
    "  - mmlu_flan_n_shot_generative_high_school_world_history": {
      "exact_match,none": 0.3206751054852321,
      "exact_match_stderr,none": 0.030381931949990424
    },
    "  - mmlu_flan_n_shot_generative_international_law": {
      "exact_match,none": 0.34710743801652894,
      "exact_match_stderr,none": 0.04345724570292534
    },
    "  - mmlu_flan_n_shot_generative_jurisprudence": {
      "exact_match,none": 0.25925925925925924,
      "exact_match_stderr,none": 0.04236511258094633
    },
    "  - mmlu_flan_n_shot_generative_logical_fallacies": {
      "exact_match,none": 0.1656441717791411,
      "exact_match_stderr,none": 0.029208296231259104
    },
    "  - mmlu_flan_n_shot_generative_moral_disputes": {
      "exact_match,none": 0.35260115606936415,
      "exact_match_stderr,none": 0.02572280220089582
    },
    "  - mmlu_flan_n_shot_generative_moral_scenarios": {
      "exact_match,none": 0.2581005586592179,
      "exact_match_stderr,none": 0.014635185616527824
    },
    "  - mmlu_flan_n_shot_generative_philosophy": {
      "exact_match,none": 0.05787781350482315,
      "exact_match_stderr,none": 0.013262604742066015
    },
    "  - mmlu_flan_n_shot_generative_prehistory": {
      "exact_match,none": 0.3333333333333333,
      "exact_match_stderr,none": 0.026229649178821163
    },
    "  - mmlu_flan_n_shot_generative_professional_law": {
      "exact_match,none": 0.28878748370273793,
      "exact_match_stderr,none": 0.011574914757219971
    },
    "  - mmlu_flan_n_shot_generative_world_religions": {
      "exact_match,none": 0.08187134502923976,
      "exact_match_stderr,none": 0.02102777265656387
    },
    " - mmlu_flan_n_shot_generative_other": {
      "exact_match,none": 0.20148052784036047,
      "exact_match_stderr,none": 0.10866247970235977
    },
    "  - mmlu_flan_n_shot_generative_business_ethics": {
      "exact_match,none": 0.45,
      "exact_match_stderr,none": 0.04999999999999999
    },
    "  - mmlu_flan_n_shot_generative_clinical_knowledge": {
      "exact_match,none": 0.17735849056603772,
      "exact_match_stderr,none": 0.023508739218846934
    },
    "  - mmlu_flan_n_shot_generative_college_medicine": {
      "exact_match,none": 0.2947976878612717,
      "exact_match_stderr,none": 0.03476599607516478
    },
    "  - mmlu_flan_n_shot_generative_global_facts": {
      "exact_match,none": 0.04,
      "exact_match_stderr,none": 0.0196946385566932
    },
    "  - mmlu_flan_n_shot_generative_human_aging": {
      "exact_match,none": 0.09417040358744394,
      "exact_match_stderr,none": 0.019602162350340534
    },
    "  - mmlu_flan_n_shot_generative_management": {
      "exact_match,none": 0.36893203883495146,
      "exact_match_stderr,none": 0.0477761518115674
    },
    "  - mmlu_flan_n_shot_generative_marketing": {
      "exact_match,none": 0.37606837606837606,
      "exact_match_stderr,none": 0.03173393632969482
    },
    "  - mmlu_flan_n_shot_generative_medical_genetics": {
      "exact_match,none": 0.07,
      "exact_match_stderr,none": 0.025643239997624283
    },
    "  - mmlu_flan_n_shot_generative_miscellaneous": {
      "exact_match,none": 0.021711366538952746,
      "exact_match_stderr,none": 0.005211631578589481
    },
    "  - mmlu_flan_n_shot_generative_nutrition": {
      "exact_match,none": 0.35947712418300654,
      "exact_match_stderr,none": 0.02747596991066095
    },
    "  - mmlu_flan_n_shot_generative_professional_accounting": {
      "exact_match,none": 0.2872340425531915,
      "exact_match_stderr,none": 0.026992199173064356
    },
    "  - mmlu_flan_n_shot_generative_professional_medicine": {
      "exact_match,none": 0.33088235294117646,
      "exact_match_stderr,none": 0.028582709753898445
    },
    "  - mmlu_flan_n_shot_generative_virology": {
      "exact_match,none": 0.16265060240963855,
      "exact_match_stderr,none": 0.028730237892613798
    },
    " - mmlu_flan_n_shot_generative_social_sciences": {
      "exact_match,none": 0.2703932401689958,
      "exact_match_stderr,none": 0.11162968540450326
    },
    "  - mmlu_flan_n_shot_generative_econometrics": {
      "exact_match,none": 0.22807017543859648,
      "exact_match_stderr,none": 0.03947152782669415
    },
    "  - mmlu_flan_n_shot_generative_high_school_geography": {
      "exact_match,none": 0.3434343434343434,
      "exact_match_stderr,none": 0.03383201223244442
    },
    "  - mmlu_flan_n_shot_generative_high_school_government_and_politics": {
      "exact_match,none": 0.35751295336787564,
      "exact_match_stderr,none": 0.03458816042181006
    },
    "  - mmlu_flan_n_shot_generative_high_school_macroeconomics": {
      "exact_match,none": 0.03076923076923077,
      "exact_match_stderr,none": 0.008755825854456833
    },
    "  - mmlu_flan_n_shot_generative_high_school_microeconomics": {
      "exact_match,none": 0.13865546218487396,
      "exact_match_stderr,none": 0.022448264476832715
    },
    "  - mmlu_flan_n_shot_generative_high_school_psychology": {
      "exact_match,none": 0.4,
      "exact_match_stderr,none": 0.021004201260420075
    },
    "  - mmlu_flan_n_shot_generative_human_sexuality": {
      "exact_match,none": 0.022900763358778626,
      "exact_match_stderr,none": 0.013119661814462918
    },
    "  - mmlu_flan_n_shot_generative_professional_psychology": {
      "exact_match,none": 0.2679738562091503,
      "exact_match_stderr,none": 0.017917974069594722
    },
    "  - mmlu_flan_n_shot_generative_public_relations": {
      "exact_match,none": 0.32727272727272727,
      "exact_match_stderr,none": 0.04494290866252089
    },
    "  - mmlu_flan_n_shot_generative_security_studies": {
      "exact_match,none": 0.3469387755102041,
      "exact_match_stderr,none": 0.030472526026726496
    },
    "  - mmlu_flan_n_shot_generative_sociology": {
      "exact_match,none": 0.3880597014925373,
      "exact_match_stderr,none": 0.03445789964362749
    },
    "  - mmlu_flan_n_shot_generative_us_foreign_policy": {
      "exact_match,none": 0.4,
      "exact_match_stderr,none": 0.04923659639173309
    },
    " - mmlu_flan_n_shot_generative_stem": {
      "exact_match,none": 0.21788772597526165,
      "exact_match_stderr,none": 0.1047441532399837
    },
    "  - mmlu_flan_n_shot_generative_abstract_algebra": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    "  - mmlu_flan_n_shot_generative_anatomy": {
      "exact_match,none": 0.14814814814814814,
      "exact_match_stderr,none": 0.030688647610352674
    },
    "  - mmlu_flan_n_shot_generative_astronomy": {
      "exact_match,none": 0.3223684210526316,
      "exact_match_stderr,none": 0.03803510248351585
    },
    "  - mmlu_flan_n_shot_generative_college_biology": {
      "exact_match,none": 0.2569444444444444,
      "exact_match_stderr,none": 0.03653946969442099
    },
    "  - mmlu_flan_n_shot_generative_college_chemistry": {
      "exact_match,none": 0.26,
      "exact_match_stderr,none": 0.044084400227680814
    },
    "  - mmlu_flan_n_shot_generative_college_computer_science": {
      "exact_match,none": 0.32,
      "exact_match_stderr,none": 0.04688261722621504
    },
    "  - mmlu_flan_n_shot_generative_college_mathematics": {
      "exact_match,none": 0.24,
      "exact_match_stderr,none": 0.042923469599092816
    },
    "  - mmlu_flan_n_shot_generative_college_physics": {
      "exact_match,none": 0.16666666666666666,
      "exact_match_stderr,none": 0.03708284662416542
    },
    "  - mmlu_flan_n_shot_generative_computer_security": {
      "exact_match,none": 0.07,
      "exact_match_stderr,none": 0.025643239997624283
    },
    "  - mmlu_flan_n_shot_generative_conceptual_physics": {
      "exact_match,none": 0.0,
      "exact_match_stderr,none": 0.0
    },
    "  - mmlu_flan_n_shot_generative_electrical_engineering": {
      "exact_match,none": 0.020689655172413793,
      "exact_match_stderr,none": 0.011861935310661093
    },
    "  - mmlu_flan_n_shot_generative_elementary_mathematics": {
      "exact_match,none": 0.25132275132275134,
      "exact_match_stderr,none": 0.022340482339643898
    },
    "  - mmlu_flan_n_shot_generative_high_school_biology": {
      "exact_match,none": 0.3709677419354839,
      "exact_match_stderr,none": 0.027480541887953593
    },
    "  - mmlu_flan_n_shot_generative_high_school_chemistry": {
      "exact_match,none": 0.23645320197044334,
      "exact_match_stderr,none": 0.02989611429173355
    },
    "  - mmlu_flan_n_shot_generative_high_school_computer_science": {
      "exact_match,none": 0.27,
      "exact_match_stderr,none": 0.04461960433384741
    },
    "  - mmlu_flan_n_shot_generative_high_school_mathematics": {
      "exact_match,none": 0.17407407407407408,
      "exact_match_stderr,none": 0.023118596033551847
    },
    "  - mmlu_flan_n_shot_generative_high_school_physics": {
      "exact_match,none": 0.33112582781456956,
      "exact_match_stderr,none": 0.038425817186598696
    },
    "  - mmlu_flan_n_shot_generative_high_school_statistics": {
      "exact_match,none": 0.32407407407407407,
      "exact_match_stderr,none": 0.03191923445686186
    },
    "  - mmlu_flan_n_shot_generative_machine_learning": {
      "exact_match,none": 0.17857142857142858,
      "exact_match_stderr,none": 0.036352091215778065
    }
  },
  "groups": {
    "mmlu_flan_n_shot_generative": {
      "exact_match,none": 0.24405355362483977,
      "exact_match_stderr,none": 0.10104199502995338
    },
    " - mmlu_flan_n_shot_generative_humanities": {
      "exact_match,none": 0.2724760892667375,
      "exact_match_stderr,none": 0.07700331605831495
    },
    " - mmlu_flan_n_shot_generative_other": {
      "exact_match,none": 0.20148052784036047,
      "exact_match_stderr,none": 0.10866247970235977
    },
    " - mmlu_flan_n_shot_generative_social_sciences": {
      "exact_match,none": 0.2703932401689958,
      "exact_match_stderr,none": 0.11162968540450326
    },
    " - mmlu_flan_n_shot_generative_stem": {
      "exact_match,none": 0.21788772597526165,
      "exact_match_stderr,none": 0.1047441532399837
    }
  },
  "configs": {
    "mmlu_flan_n_shot_generative_abstract_algebra": {
      "task": "mmlu_flan_n_shot_generative_abstract_algebra",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "abstract_algebra",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about abstract algebra.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_anatomy": {
      "task": "mmlu_flan_n_shot_generative_anatomy",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "anatomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about anatomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_astronomy": {
      "task": "mmlu_flan_n_shot_generative_astronomy",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "astronomy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about astronomy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_business_ethics": {
      "task": "mmlu_flan_n_shot_generative_business_ethics",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "business_ethics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about business ethics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_clinical_knowledge": {
      "task": "mmlu_flan_n_shot_generative_clinical_knowledge",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "clinical_knowledge",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about clinical knowledge.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_college_biology": {
      "task": "mmlu_flan_n_shot_generative_college_biology",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_college_chemistry": {
      "task": "mmlu_flan_n_shot_generative_college_chemistry",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_college_computer_science": {
      "task": "mmlu_flan_n_shot_generative_college_computer_science",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_college_mathematics": {
      "task": "mmlu_flan_n_shot_generative_college_mathematics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_college_medicine": {
      "task": "mmlu_flan_n_shot_generative_college_medicine",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_college_physics": {
      "task": "mmlu_flan_n_shot_generative_college_physics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "college_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about college physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_computer_security": {
      "task": "mmlu_flan_n_shot_generative_computer_security",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "computer_security",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about computer security.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_conceptual_physics": {
      "task": "mmlu_flan_n_shot_generative_conceptual_physics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "conceptual_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about conceptual physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_econometrics": {
      "task": "mmlu_flan_n_shot_generative_econometrics",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "econometrics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about econometrics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_electrical_engineering": {
      "task": "mmlu_flan_n_shot_generative_electrical_engineering",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "electrical_engineering",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about electrical engineering.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_elementary_mathematics": {
      "task": "mmlu_flan_n_shot_generative_elementary_mathematics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "elementary_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about elementary mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_formal_logic": {
      "task": "mmlu_flan_n_shot_generative_formal_logic",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "formal_logic",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about formal logic.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_global_facts": {
      "task": "mmlu_flan_n_shot_generative_global_facts",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "global_facts",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about global facts.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_biology": {
      "task": "mmlu_flan_n_shot_generative_high_school_biology",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_biology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school biology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_chemistry": {
      "task": "mmlu_flan_n_shot_generative_high_school_chemistry",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_chemistry",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school chemistry.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_computer_science": {
      "task": "mmlu_flan_n_shot_generative_high_school_computer_science",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_computer_science",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school computer science.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_european_history": {
      "task": "mmlu_flan_n_shot_generative_high_school_european_history",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_european_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school european history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_geography": {
      "task": "mmlu_flan_n_shot_generative_high_school_geography",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_geography",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school geography.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_government_and_politics": {
      "task": "mmlu_flan_n_shot_generative_high_school_government_and_politics",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_government_and_politics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school government and politics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_macroeconomics": {
      "task": "mmlu_flan_n_shot_generative_high_school_macroeconomics",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_macroeconomics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school macroeconomics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_mathematics": {
      "task": "mmlu_flan_n_shot_generative_high_school_mathematics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_mathematics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school mathematics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_microeconomics": {
      "task": "mmlu_flan_n_shot_generative_high_school_microeconomics",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_microeconomics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school microeconomics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_physics": {
      "task": "mmlu_flan_n_shot_generative_high_school_physics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_physics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school physics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_psychology": {
      "task": "mmlu_flan_n_shot_generative_high_school_psychology",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school psychology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_statistics": {
      "task": "mmlu_flan_n_shot_generative_high_school_statistics",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_statistics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school statistics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_us_history": {
      "task": "mmlu_flan_n_shot_generative_high_school_us_history",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_us_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school us history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_high_school_world_history": {
      "task": "mmlu_flan_n_shot_generative_high_school_world_history",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "high_school_world_history",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about high school world history.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_human_aging": {
      "task": "mmlu_flan_n_shot_generative_human_aging",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "human_aging",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about human aging.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_human_sexuality": {
      "task": "mmlu_flan_n_shot_generative_human_sexuality",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "human_sexuality",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about human sexuality.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_international_law": {
      "task": "mmlu_flan_n_shot_generative_international_law",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "international_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about international law.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_jurisprudence": {
      "task": "mmlu_flan_n_shot_generative_jurisprudence",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "jurisprudence",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about jurisprudence.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_logical_fallacies": {
      "task": "mmlu_flan_n_shot_generative_logical_fallacies",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "logical_fallacies",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about logical fallacies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_machine_learning": {
      "task": "mmlu_flan_n_shot_generative_machine_learning",
      "group": "mmlu_flan_n_shot_generative_stem",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "machine_learning",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about machine learning.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_management": {
      "task": "mmlu_flan_n_shot_generative_management",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "management",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about management.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_marketing": {
      "task": "mmlu_flan_n_shot_generative_marketing",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "marketing",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about marketing.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_medical_genetics": {
      "task": "mmlu_flan_n_shot_generative_medical_genetics",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "medical_genetics",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about medical genetics.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_miscellaneous": {
      "task": "mmlu_flan_n_shot_generative_miscellaneous",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "miscellaneous",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about miscellaneous.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_moral_disputes": {
      "task": "mmlu_flan_n_shot_generative_moral_disputes",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "moral_disputes",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about moral disputes.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_moral_scenarios": {
      "task": "mmlu_flan_n_shot_generative_moral_scenarios",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "moral_scenarios",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about moral scenarios.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_nutrition": {
      "task": "mmlu_flan_n_shot_generative_nutrition",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "nutrition",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about nutrition.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_philosophy": {
      "task": "mmlu_flan_n_shot_generative_philosophy",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "philosophy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about philosophy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_prehistory": {
      "task": "mmlu_flan_n_shot_generative_prehistory",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "prehistory",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about prehistory.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_professional_accounting": {
      "task": "mmlu_flan_n_shot_generative_professional_accounting",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_accounting",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about professional accounting.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_professional_law": {
      "task": "mmlu_flan_n_shot_generative_professional_law",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_law",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about professional law.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_professional_medicine": {
      "task": "mmlu_flan_n_shot_generative_professional_medicine",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_medicine",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about professional medicine.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_professional_psychology": {
      "task": "mmlu_flan_n_shot_generative_professional_psychology",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "professional_psychology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about professional psychology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_public_relations": {
      "task": "mmlu_flan_n_shot_generative_public_relations",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "public_relations",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about public relations.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_security_studies": {
      "task": "mmlu_flan_n_shot_generative_security_studies",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "security_studies",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about security studies.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_sociology": {
      "task": "mmlu_flan_n_shot_generative_sociology",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "sociology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about sociology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_us_foreign_policy": {
      "task": "mmlu_flan_n_shot_generative_us_foreign_policy",
      "group": "mmlu_flan_n_shot_generative_social_sciences",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "us_foreign_policy",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about us foreign policy.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_virology": {
      "task": "mmlu_flan_n_shot_generative_virology",
      "group": "mmlu_flan_n_shot_generative_other",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "virology",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about virology.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    },
    "mmlu_flan_n_shot_generative_world_religions": {
      "task": "mmlu_flan_n_shot_generative_world_religions",
      "group": "mmlu_flan_n_shot_generative_humanities",
      "dataset_path": "hails/mmlu_no_train",
      "dataset_name": "world_religions",
      "test_split": "test",
      "fewshot_split": "dev",
      "doc_to_text": "Q: {{question.strip()}}\n(A) {{choices[0]}} (B) {{choices[1]}} (C) {{choices[2]}} (D) {{choices[3]}}\nA: ",
      "doc_to_target": "{{['(A)', '(B)', '(C)', '(D)'][answer]}}",
      "description": "The following are multiple choice questions (with answers) about world religions.\n\n",
      "target_delimiter": " ",
      "fewshot_delimiter": "\n\n",
      "num_fewshot": 5,
      "metric_list": [
        {
          "metric": "exact_match",
          "aggregation": "mean",
          "higher_is_better": true
        }
      ],
      "output_type": "generate_until",
      "generation_kwargs": {
        "until": [
          "</s>"
        ]
      },
      "repeats": 1,
      "should_decontaminate": false
    }
  },
  "versions": {
    "  - mmlu_flan_n_shot_generative_abstract_algebra": "Yaml",
    "  - mmlu_flan_n_shot_generative_anatomy": "Yaml",
    "  - mmlu_flan_n_shot_generative_astronomy": "Yaml",
    "  - mmlu_flan_n_shot_generative_business_ethics": "Yaml",
    "  - mmlu_flan_n_shot_generative_clinical_knowledge": "Yaml",
    "  - mmlu_flan_n_shot_generative_college_biology": "Yaml",
    "  - mmlu_flan_n_shot_generative_college_chemistry": "Yaml",
    "  - mmlu_flan_n_shot_generative_college_computer_science": "Yaml",
    "  - mmlu_flan_n_shot_generative_college_mathematics": "Yaml",
    "  - mmlu_flan_n_shot_generative_college_medicine": "Yaml",
    "  - mmlu_flan_n_shot_generative_college_physics": "Yaml",
    "  - mmlu_flan_n_shot_generative_computer_security": "Yaml",
    "  - mmlu_flan_n_shot_generative_conceptual_physics": "Yaml",
    "  - mmlu_flan_n_shot_generative_econometrics": "Yaml",
    "  - mmlu_flan_n_shot_generative_electrical_engineering": "Yaml",
    "  - mmlu_flan_n_shot_generative_elementary_mathematics": "Yaml",
    "  - mmlu_flan_n_shot_generative_formal_logic": "Yaml",
    "  - mmlu_flan_n_shot_generative_global_facts": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_biology": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_chemistry": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_computer_science": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_european_history": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_geography": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_government_and_politics": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_macroeconomics": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_mathematics": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_microeconomics": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_physics": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_psychology": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_statistics": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_us_history": "Yaml",
    "  - mmlu_flan_n_shot_generative_high_school_world_history": "Yaml",
    "  - mmlu_flan_n_shot_generative_human_aging": "Yaml",
    "  - mmlu_flan_n_shot_generative_human_sexuality": "Yaml",
    "  - mmlu_flan_n_shot_generative_international_law": "Yaml",
    "  - mmlu_flan_n_shot_generative_jurisprudence": "Yaml",
    "  - mmlu_flan_n_shot_generative_logical_fallacies": "Yaml",
    "  - mmlu_flan_n_shot_generative_machine_learning": "Yaml",
    "  - mmlu_flan_n_shot_generative_management": "Yaml",
    "  - mmlu_flan_n_shot_generative_marketing": "Yaml",
    "  - mmlu_flan_n_shot_generative_medical_genetics": "Yaml",
    "  - mmlu_flan_n_shot_generative_miscellaneous": "Yaml",
    "  - mmlu_flan_n_shot_generative_moral_disputes": "Yaml",
    "  - mmlu_flan_n_shot_generative_moral_scenarios": "Yaml",
    "  - mmlu_flan_n_shot_generative_nutrition": "Yaml",
    "  - mmlu_flan_n_shot_generative_philosophy": "Yaml",
    "  - mmlu_flan_n_shot_generative_prehistory": "Yaml",
    "  - mmlu_flan_n_shot_generative_professional_accounting": "Yaml",
    "  - mmlu_flan_n_shot_generative_professional_law": "Yaml",
    "  - mmlu_flan_n_shot_generative_professional_medicine": "Yaml",
    "  - mmlu_flan_n_shot_generative_professional_psychology": "Yaml",
    "  - mmlu_flan_n_shot_generative_public_relations": "Yaml",
    "  - mmlu_flan_n_shot_generative_security_studies": "Yaml",
    "  - mmlu_flan_n_shot_generative_sociology": "Yaml",
    "  - mmlu_flan_n_shot_generative_us_foreign_policy": "Yaml",
    "  - mmlu_flan_n_shot_generative_virology": "Yaml",
    "  - mmlu_flan_n_shot_generative_world_religions": "Yaml",
    " - mmlu_flan_n_shot_generative_humanities": "N/A",
    " - mmlu_flan_n_shot_generative_other": "N/A",
    " - mmlu_flan_n_shot_generative_social_sciences": "N/A",
    " - mmlu_flan_n_shot_generative_stem": "N/A",
    "mmlu_flan_n_shot_generative": "N/A"
  },
  "config": {
    "model": "hf",
    "model_args": "pretrained=/fsx/lintangsutawika/00-improved-t5/checkpoints/hf/t5-v2-base-2M-flan",
    "batch_size": "4",
    "batch_sizes": [],
    "device": null,
    "use_cache": null,
    "limit": null,
    "bootstrap_iters": 100000
  },
  "git_hash": "b439c98"
}