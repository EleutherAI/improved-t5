from __gin__ import dynamic_registration

import __main__ as train_script
import seqio
from t5x import utils
from t5x import models
from t5x import trainer
from t5x import partitioning

import data.pile.tasks

include 't5x/configs/runs/finetune.gin'

MIXTURE_OR_TASK_NAME = "pile_s_causal"
TASK_FEATURE_LENGTHS = {"inputs": 2048, "targets": 2048}
BATCH_SIZE = 512
EVAL_BATCH_SIZE = 256
DROPOUT_RATE = 0.0

train_script.train:
  run_eval_before_training = True
  eval_steps = 100
  eval_period = %SAVING_PERIOD
  infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()
  inference_evaluator_cls = @seqio.Evaluator

infer_eval/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'validation'
  batch_size = %EVAL_BATCH_SIZE
  seed = 0
  shuffle = False

seqio.Evaluator:
  logger_cls = [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]
  num_examples = 2048 # Use all examples in the infer_eval dataset.

utils.SaveCheckpointConfig:
  keep = None
  period = %SAVING_PERIOD

trainer.Trainer:
  learning_rate_fn = @utils.create_learning_rate_scheduler()

utils.create_learning_rate_scheduler:
  factors = 'constant * rsqrt_decay'
  base_learning_rate = 1.0
  warmup_steps = 10000  # 10k to keep consistent with T5/MTF defaults.