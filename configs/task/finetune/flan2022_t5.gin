from __gin__ import dynamic_registration

import __main__ as train_script
from t5x import utils
from t5x import partitioning

import seqio
import data.flan.tasks

include 't5x/configs/runs/finetune.gin'

MIXTURE_OR_TASK_NAME = "flan2022_submix_t5"
TASK_FEATURE_LENGTHS = {'inputs': 2048, 'targets': 512}

DROPOUT_RATE = 0.05
BATCH_SIZE = 64
EVAL_STEPS = 10
EVAL_PERIOD = %SAVING_PERIOD
LEARNING_RATE = 0.0005

utils.SaveCheckpointConfig:
  period = %SAVING_PERIOD

train_script.train:
  run_eval_before_training = False
  eval_steps = %EVAL_STEPS
  eval_period = %SAVING_PERIOD
  infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()
  inference_evaluator_cls = @seqio.Evaluator
  partitioner = @partitioning.PjitPartitioner()

partitioning.PjitPartitioner:
  num_partitions = 1
  model_parallel_submesh = (4,2,2,1)
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

train_eval/utils.DatasetConfig:
  split = 'train'

infer_eval/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'train'
  batch_size = %BATCH_SIZE
  seed = 0
  shuffle = False

seqio.Evaluator:
  logger_cls = [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]

utils.create_learning_rate_scheduler.base_learning_rate = %LEARNING_RATE