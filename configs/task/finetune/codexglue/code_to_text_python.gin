from __gin__ import dynamic_registration

import __main__ as train_script
from t5x import utils
from t5x import partitioning

import seqio
import data.codexglue.tasks

include 't5x/configs/runs/finetune.gin'

MIXTURE_OR_TASK_NAME = "code_to_text_python"
TASK_FEATURE_LENGTHS = {'inputs': 256, 'targets': 128}

DROPOUT_RATE = 0.1
BATCH_SIZE = 32
EVAL_STEPS = 10
EVAL_PERIOD = %SAVING_PERIOD
LEARNING_RATE = 5e-5

utils.SaveCheckpointConfig:
  period = %SAVING_PERIOD

train_script.train:
  run_eval_before_training = True
  eval_steps = %EVAL_STEPS
  eval_period = %SAVING_PERIOD
  infer_eval_dataset_cfg = @infer_eval/utils.DatasetConfig()
  inference_evaluator_cls = @seqio.Evaluator
  partitioner = @partitioning.PjitPartitioner()

partitioning.PjitPartitioner:
  num_partitions = 2
  model_parallel_submesh = None
  logical_axis_rules = @partitioning.standard_logical_axis_rules()

infer_eval/utils.DatasetConfig:
  mixture_or_task_name = %MIXTURE_OR_TASK_NAME
  task_feature_lengths = %TASK_FEATURE_LENGTHS
  split = 'validation'
  batch_size = %BATCH_SIZE
  seed = 0
  shuffle = False

seqio.Evaluator:
  logger_cls = [@seqio.PyLoggingLogger, @seqio.TensorBoardLogger, @seqio.JSONLogger]
  num_examples = 2048 # Use all examples in the infer_eval dataset.

utils.create_learning_rate_scheduler.base_learning_rate = %LEARNING_RATE