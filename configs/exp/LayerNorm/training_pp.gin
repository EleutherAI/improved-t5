from __gin__ import dynamic_registration

import optax

from t5x import utils
from t5x import optimizers

BATCH_SIZE = 2048
DROPOUT_RATE = 0.1

# ------------------- Optimizer ------------------------------------------------
# `learning_rate` is set by `Trainer.learning_rate_fn`.
OPTIMIZER = @optimizers.chain()

optimizers.chain:
  transformations = [@optax.clip(), @optax.adamw()]

optax.clip:
  max_delta = 2.0

optax.adamw:
  learning_rate = @utils.create_learning_rate_scheduler()
  eps = 1e-06
  b1 = 0.9
  b2 = 0.98
  weight_decay = 0.01

utils.create_learning_rate_scheduler:
  factors = 'linear_decay'
  decay_factor = 8e-06
  base_learning_rate = 4e-04
  warmup_steps = 10000
