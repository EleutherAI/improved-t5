from __gin__ import dynamic_registration

import optax

from t5x import utils
from t5x import optimizers

from models.scalable_t5 import network

BATCH_SIZE = 2048
DROPOUT_RATE = 0.1
utils.SaveCheckpointConfig:
  keep = None
  period = %SAVING_PERIOD

# ------------------- Optimizer ------------------------------------------------
# `learning_rate` is set by `Trainer.learning_rate_fn`.
OPTIMIZER = @optimizers.chain()

optimizers.chain:
  # transformations = [@optax.clip(), @optax.adamw()]
  transformations = [@optax.adamw()]

# optax.clip:
#   max_delta = 2.0

optax.adamw:
  learning_rate = @utils.create_learning_rate_scheduler()
  eps = 1e-06
  b1 = 0.9
  b2 = 0.98
  weight_decay = 0.01

utils.create_learning_rate_scheduler:
  factors = 'linear_decay'
  decay_factor = 8e-06
  base_learning_rate = 4e-04
  warmup_steps = 10000

# ------------------- Network specification ------------------------------------
network.T5Config:
    use_alibi = True
    use_rel_pos = True
    normalize_before = False
